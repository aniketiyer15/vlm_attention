{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a68b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"ENTER-KEY-HERE\"\n",
    "ANTHROPIC_API_KEY = \"ENTER-KEY-HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"pipeline_images\"           \n",
    "OUTPUT_PATH = \"captions.jsonl\"    \n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369719b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95963f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babda430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "• Must accurately describe the visible scene.\n",
    "• 7–15 words, objective, simple, and factual.\n",
    "• Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "• Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "• Must keep the same length and sentence structure style as the correct caption.\n",
    "• MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     – species/category of the main object\n",
    "     – color of a main object\n",
    "     – pattern/texture of a main object\n",
    "     – object type that a person is holding/using\n",
    "     – action the main subject is performing\n",
    "     – spatial relation (e.g., “in front of” → “behind”)\n",
    "     \n",
    "• The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, “ocean” is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "• The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "• The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0–L4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 — Pure language prior  \n",
    "• Must be answerable with NO access to the image.  \n",
    "• General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "• 6–14 words.\n",
    "\n",
    "L1 — Probe changed attribute #1 \n",
    "• MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "• Example:If species changed, ask “What type of animal…?”  \n",
    "          If color changed, ask “What color is…?”  \n",
    "          If object type changed, ask “What object is… holding?”  \n",
    "• No attributes other than the first changed one.  \n",
    "• 6–14 words.\n",
    "\n",
    "L2 — Probe changed attribute #2\n",
    "• MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "• Same rules as L2 but targeting the second changed detail.  \n",
    "• Should not be the same question as L1. \n",
    "• 6–14 words.\n",
    "\n",
    "L3 — High-level reasoning\n",
    "• Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "• The question MUST NOT depend on the two changed attributes.\n",
    "• The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "• The question SHOULD require general common-sense or contextual reasoning.\n",
    "• The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "• 6–14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "• Do NOT provide answers.\n",
    "• Do NOT describe the image outside captions.\n",
    "• All questions must be 6–14 words.\n",
    "• Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_blip2(image_path, caption, question, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Runs BLIP-2 Flan-T5-xl on:\n",
    "        IMAGE + (caption + question) text prompt.\n",
    "    Returns: the generated answer as a clean string.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Load image ----\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---- Build prompt for BLIP-2 ----\n",
    "    # Format:  \"<caption>\\n\\nQuestion: <question>\\nAnswer:\"\n",
    "    prompt = f\"{caption}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    # ---- Generate answer ----\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,      # deterministic & stable (recommended)\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # ---- Decode ----\n",
    "    answer = processor.tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # ---- Strip the prompt part (BLIP-2 often echoes input) ----\n",
    "    if \"Answer:\" in answer:\n",
    "        answer = answer.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "    # Clean spacing\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c327ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "# \"Salesforce/blip2-flan-t5-xxl\" -> bigger, might need 2x memory\n",
    "MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffa85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to setup eval metric calculation\n",
    "\n",
    "def pair_stats_by_level(jsonl_path):\n",
    "    levels = [\"L0\", \"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "    # Tallies per level\n",
    "    pair_stats = {\n",
    "        lvl: {(1,1):0, (1,0):0, (0,1):0, (0,0):0}\n",
    "        for lvl in levels\n",
    "    }\n",
    "\n",
    "    # ---- Single JSONL pass ----\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            for lvl in levels:\n",
    "                s_c = eval_scores[lvl][\"correct_caption_score\"]\n",
    "                s_i = eval_scores[lvl][\"incorrect_caption_score\"]\n",
    "                pair_stats[lvl][(s_c, s_i)] += 1\n",
    "\n",
    "    return pair_stats\n",
    "\n",
    "def conf_pairs_by_level(pair_stats):\n",
    "    return pair_stats  # already exactly the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metric - fooling rate by level \n",
    "# When the model answers correctly in the correct-caption condition but answers incorrectly in the incorrect-caption condition.\n",
    "\n",
    "def fooling_rate_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c10 = counts[(1,0)]\n",
    "        total = sum(counts.values())\n",
    "        rate = c10 / total if total > 0 else 0\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"fooled\": c10,\n",
    "            \"total\": total,\n",
    "            \"rate\": rate,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metrics - per-level answer accuracy and MDI, computed separately for the correct-caption and incorrect-caption conditions.\n",
    "\n",
    "def acc_mdi_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c11 = counts[(1,1)]\n",
    "        c10 = counts[(1,0)]\n",
    "        c01 = counts[(0,1)]\n",
    "        c00 = counts[(0,0)]\n",
    "        total = c11 + c10 + c01 + c00\n",
    "\n",
    "        if total == 0:\n",
    "            results[lvl] = None\n",
    "            continue\n",
    "\n",
    "        # accuracy under correct caption = model is correct (regardless of incorrect-caption score)\n",
    "        acc_correct = (c11 + c10) / total\n",
    "        # accuracy under incorrect caption = model is correct under wrong caption\n",
    "        acc_incorrect = (c11 + c01) / total\n",
    "\n",
    "        mdi = acc_correct - acc_incorrect\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"accuracy_correct_caption\": acc_correct,\n",
    "            \"accuracy_incorrect_caption\": acc_incorrect,\n",
    "            \"MDI\": mdi,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_BLIP_outputs():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, correct_caption, L3)\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, incorrect_caption, L3)\n",
    "                }\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"eval_scores\": {}   # will be filled next\n",
    "                }\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ######### BLIP #########\n",
    "    \n",
    "    # Generates dataset used (correct/incorrect captions, L0-L4 questions)\n",
    "    # Evaluates BLIP-2 responses via Claude Sonnet 4.5 (0 - incorrect; 1 - correct)\n",
    "    generate_BLIP_outputs() \n",
    "    \n",
    "    # Compute metrics for BLIP responses\n",
    "    BLIP_pair_stats = pair_stats_by_level(OUTPUT_PATH)\n",
    "    BLIP_fooling_rate_per_level = fooling_rate_by_level(BLIP_pair_stats)\n",
    "    BLIP_acc_mdi_per_level = acc_mdi_by_level(BLIP_pair_stats)\n",
    "    \n",
    "    print(\"\\n========================\")\n",
    "    print(\"FOOLING RATE PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in BLIP_fooling_rate_per_level.items():\n",
    "        print(f\"{lvl}: Fooling Rate = {stats['fooled']}/{stats['total']} \"\n",
    "              f\"({stats['rate']:.2f})\")\n",
    "\n",
    "    print(\"\\n========================\")\n",
    "    print(\"ACCURACY + MDI PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in BLIP_acc_mdi_per_level.items():\n",
    "        print(f\"{lvl}:  \"\n",
    "            f\"Acc(correct caption) = {stats['accuracy_correct_caption']:.2f},  \"\n",
    "            f\"Acc(incorrect caption) = {stats['accuracy_incorrect_caption']:.2f},  \"\n",
    "            f\"MDI = {stats['MDI']:.2f}\")\n",
    "         \n",
    "    ######## LLAVA ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c41954",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
