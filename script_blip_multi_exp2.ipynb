{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef566c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416496ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "BLIP_OUTPUT_PATH = \"blip_multi_exp2_responses.jsonl\"  \n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666f337",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae041b93",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "============================================================\n",
    "1. Correct Caption\n",
    "============================================================\n",
    "• Accurately describe the visible scene.\n",
    "• 9–15 words, objective, simple, and factual.\n",
    "• Should mention main objects; avoid inference beyond evidence.\n",
    "\n",
    "============================================================\n",
    "2. Visual Necessity Question Ladder (VNL): Levels L0 → L4\n",
    "============================================================\n",
    "\n",
    "GENERAL RULES:\n",
    "• L1–L4 MUST require looking at the image to answer.\n",
    "• All questions MUST be answerable using only the given image.\n",
    "• Do NOT include the answers.\n",
    "• No question should exceed 14 words.\n",
    "• Return concise, natural wording.\n",
    "\n",
    "------------------------------------------------------------\n",
    "L0 – Baseline Question (Language-prior only)\n",
    "------------------------------------------------------------\n",
    "• A question humans can answer **without seeing the image**.\n",
    "• May refer to the world generally (NOT the specific image).\n",
    "• Purpose: control for language-only biases.\n",
    "• 6–12 words.\n",
    "Examples:\n",
    "– “What season often has the coldest weather?”  \n",
    "– “Which animal is larger, a dog or an elephant?”  \n",
    "– “What do people usually use to take photographs?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L1 – Basic Visual Recognition\n",
    "------------------------------------------------------------\n",
    "• Requires the image.\n",
    "• Ask about a **primary object** or its basic property.\n",
    "• No reasoning, no inference.\n",
    "Examples:\n",
    "– “What object is the person holding?”  \n",
    "– “What color is the animal?”  \n",
    "– “How many people are visible?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L2 – Intermediate Visual Detail\n",
    "------------------------------------------------------------\n",
    "• Also requires the image.\n",
    "• Ask about a **secondary property** of a main object.\n",
    "• Slightly more specific than L1.\n",
    "Examples:\n",
    "– “What pattern is on the person’s shirt?”  \n",
    "– “What type of hat is the man wearing?”  \n",
    "– “What material is the table made of?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L3 – Relational / Spatial Reasoning\n",
    "------------------------------------------------------------\n",
    "• Requires image + spatial relations + relational understanding.\n",
    "Examples:\n",
    "– “Where is the dog positioned relative to the child?”  \n",
    "– “What object is behind the bicycle?”  \n",
    "– “Which person is closest to the camera?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L4 – High-Level Visual Reasoning\n",
    "------------------------------------------------------------\n",
    "• Hardest level; requires the entire scene.\n",
    "• Ask about interactions, goals, implied roles, or multi-object context.\n",
    "• Still must be answerable from the image alone (no external inference).\n",
    "Examples:\n",
    "– “What activity are the people engaged in?”  \n",
    "– “Why is the man extending his arm?”  \n",
    "– “What is the group collectively doing?”\n",
    "\n",
    "============================================================\n",
    "Return EXACTLY this JSON structure:\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",\n",
    "  \"L2\": \"<string>\",\n",
    "  \"L3\": \"<string>\",\n",
    "  \"L4\": \"<string>\"\n",
    "}\n",
    "============================================================\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 ADDED - in version to compute attention-based MDI \n",
    "\n",
    "def compute_mdi(attentions, n_img_tokens=32):\n",
    "    \"\"\"\n",
    "    Compute Modality Dominance Index (MDI) from BLIP-2 cross-attention tensors.\n",
    "\n",
    "    attentions:\n",
    "        - tuple of (num_layers,) where each element is either:\n",
    "              Tensor[batch, heads, tgt_len, src_len]\n",
    "          OR nested tuples depending on HF version.\n",
    "    n_img_tokens: number of image tokens in the cross-attention source sequence.\n",
    "\n",
    "    Returns:\n",
    "        Single float MDI value:\n",
    "            visual_attn / (visual_attn + textual_attn + 1e-9)\n",
    "        or None if no valid attention tensors were found.\n",
    "    \"\"\"\n",
    "\n",
    "    flat_attns = []\n",
    "\n",
    "    # ---- 1. Flatten nested tuples ----\n",
    "    if isinstance(attentions, (list, tuple)):\n",
    "        for a in attentions:\n",
    "            if isinstance(a, (list, tuple)):\n",
    "                flat_attns.extend([\n",
    "                    x for x in a if isinstance(x, torch.Tensor)\n",
    "                ])\n",
    "            elif isinstance(a, torch.Tensor):\n",
    "                flat_attns.append(a)\n",
    "    elif isinstance(attentions, torch.Tensor):\n",
    "        flat_attns.append(attentions)\n",
    "\n",
    "    if not flat_attns:\n",
    "        print(\"⚠️  No attention tensors found.\")\n",
    "        return None\n",
    "\n",
    "    visual_scores = []\n",
    "    textual_scores = []\n",
    "\n",
    "    # ---- 2. Compute visual/textual attention for each layer ----\n",
    "    for layer_attn in flat_attns:\n",
    "        if not isinstance(layer_attn, torch.Tensor):\n",
    "            continue\n",
    "\n",
    "        # layer_attn shape: [batch, heads, tgt_len, src_len]\n",
    "        # average over batch + heads\n",
    "        attn_mean = layer_attn.mean(dim=(0, 1))  # -> [tgt_len, src_len]\n",
    "\n",
    "        tgt_len, src_len = attn_mean.shape\n",
    "\n",
    "        # safety check\n",
    "        n_img_tokens_safe = min(n_img_tokens, src_len)\n",
    "\n",
    "        # first n tokens = image tokens\n",
    "        visual = attn_mean[:, :n_img_tokens_safe].mean().item()\n",
    "\n",
    "        # rest = text tokens\n",
    "        textual = attn_mean[:, n_img_tokens_safe:].mean().item()\n",
    "\n",
    "        visual_scores.append(visual)\n",
    "        textual_scores.append(textual)\n",
    "\n",
    "    if not visual_scores:\n",
    "        return None\n",
    "\n",
    "    # Average over layers\n",
    "    visual_avg = sum(visual_scores) / len(visual_scores)\n",
    "    textual_avg = sum(textual_scores) / len(textual_scores)\n",
    "\n",
    "    # ---- 3. Modality Dominance Index ----\n",
    "    mdi = visual_avg / (visual_avg + textual_avg + 1e-9)\n",
    "\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 (Only BLIP compatibility)\n",
    "\n",
    "def flatten_attn_tensors(attentions):\n",
    "    \"\"\"\n",
    "    Flattens nested BLIP-2 attention structures into a simple list of tensors.\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "\n",
    "    if isinstance(attentions, (list, tuple)):\n",
    "        for a in attentions:\n",
    "            if isinstance(a, (list, tuple)):\n",
    "                flat.extend([x for x in a if isinstance(x, torch.Tensor)])\n",
    "            elif isinstance(a, torch.Tensor):\n",
    "                flat.append(a)\n",
    "    elif isinstance(attentions, torch.Tensor):\n",
    "        flat.append(attentions)\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6290230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "#V1 (Only BLIP compatibility)\n",
    "\n",
    "def compute_attention_entropy(attentions):\n",
    "    \"\"\"\n",
    "    Compute entropy of BLIP-2 decoder/cross-attention tensors.\n",
    "    \"\"\"\n",
    "    flat = flatten_attn_tensors(attentions)\n",
    "    if not flat:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer in flat:\n",
    "        if not isinstance(layer, torch.Tensor):\n",
    "            continue\n",
    "        if layer.numel() == 0:     # <-- important fix\n",
    "            continue\n",
    "\n",
    "        # Normalize to [batch, heads, tgt, src]\n",
    "        if layer.dim() == 3:\n",
    "            layer = layer.unsqueeze(0)\n",
    "\n",
    "        logits = layer.float()                # [batch, heads, tgt, src]\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        probs  = probs.clamp(min=1e-9)\n",
    "\n",
    "        entropy = -(probs * probs.log()).sum(dim=-1)   # [batch, heads, tgt]\n",
    "        entropies.append(entropy.mean().item())        # scalar\n",
    "\n",
    "    return sum(entropies) / len(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b456b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pad the src_len dimension so that 'a' and 'b' have the same shape.\n",
    "    Padding is applied on the last dimension (src_len).\n",
    "    Shapes expected: [batch, heads, tgt_len, src_len].\n",
    "    \"\"\"\n",
    "    if a.size(-1) == b.size(-1):\n",
    "        return a, b\n",
    "\n",
    "    diff = a.size(-1) - b.size(-1)\n",
    "\n",
    "    if diff > 0:\n",
    "        # a is longer — pad b\n",
    "        pad = (0, diff)  # pad right side of src_len\n",
    "        b = torch.nn.functional.pad(b, pad)\n",
    "    else:\n",
    "        # b is longer — pad a\n",
    "        pad = (0, -diff)\n",
    "        a = torch.nn.functional.pad(a, pad)\n",
    "\n",
    "    return a, b\n",
    "\n",
    "#V1 (Only BLIP compatibility)\n",
    "\n",
    "def compute_attention_shift(prev_attn, curr_attn):\n",
    "    \"\"\"\n",
    "    Compute average L1 shift between two sets of attention tensors.\n",
    "    Handles nested tuples, 3D/4D mismatches, and differing src_len.\n",
    "    \"\"\"\n",
    "    prev_flat = flatten_attn_tensors(prev_attn)\n",
    "    curr_flat = flatten_attn_tensors(curr_attn)\n",
    "\n",
    "    shifts = []\n",
    "\n",
    "    for A, B in zip(prev_flat, curr_flat):\n",
    "        if not (isinstance(A, torch.Tensor) and isinstance(B, torch.Tensor)):\n",
    "            continue\n",
    "        if A.numel() == 0 or B.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Normalize 3D → 4D: [heads, tgt, src] → [1, heads, tgt, src]\n",
    "        if A.dim() == 3:\n",
    "            A = A.unsqueeze(0)\n",
    "        if B.dim() == 3:\n",
    "            B = B.unsqueeze(0)\n",
    "\n",
    "        # src_len might differ → pad\n",
    "        A, B = pad_to_match(A, B)\n",
    "\n",
    "        # Compute shift: mean absolute difference\n",
    "        shift = torch.abs(A - B).mean().item()\n",
    "        shifts.append(shift)\n",
    "\n",
    "    if not shifts:\n",
    "        return None\n",
    "\n",
    "    return sum(shifts) / len(shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5680",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Multi-turn support\n",
    "\n",
    "def ask_blip2(\n",
    "    path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_mdi=False,\n",
    "    return_attn=False,\n",
    "    last_turn_only=False\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # ===== Build BLIP-2-Compatible Prompt =====\n",
    "    prompt_parts = []\n",
    "\n",
    "    # Caption\n",
    "    prompt_parts.append(f\"Caption: {caption}\\n\\n\")\n",
    "\n",
    "\n",
    "    if len(history) > 0:\n",
    "        prompt_parts.append(\"Previous QA:\\n\")\n",
    "\n",
    "# Don't need - generate_BLIP_outputs() take care of storing history depending on last_turn_only flag passed in\n",
    "#         if last_turn_only:\n",
    "#             # use ONLY last turn\n",
    "#             q_prev, a_prev = history[-1]\n",
    "#             prompt_parts.append(f\"Question: {q_prev}\\n\")\n",
    "#             prompt_parts.append(f\"Answer: {a_prev}\\n\\n\")\n",
    "#         else:\n",
    "#             # append ALL history turns\n",
    "        for q_prev, a_prev in history:\n",
    "            prompt_parts.append(f\"Question: {q_prev}\\n\")\n",
    "            prompt_parts.append(f\"Answer: {a_prev}\\n\\n\")\n",
    "        \n",
    "\n",
    "    # Current question\n",
    "    prompt_parts.append(f\"Question: {question}\\n\")\n",
    "    prompt_parts.append(\"Answer: \")\n",
    "\n",
    "    prompt = \"\".join(prompt_parts)\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, torch.float16)\n",
    "\n",
    "    # FAST PATH (no attentions)\n",
    "    if not return_mdi and not return_attn:\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "        answer = processor.tokenizer.decode(\n",
    "            output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer after \"Answer:\"\n",
    "        if \"Answer:\" in answer:\n",
    "            answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "        return answer\n",
    "\n",
    "    # ---- SLOW PATH (with attentions) ----\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True\n",
    "    )\n",
    "\n",
    "    answer = processor.tokenizer.decode(\n",
    "        outputs.sequences[0], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    if \"Answer:\" in answer:\n",
    "        answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Extract attentions\n",
    "    attns = (\n",
    "        outputs.cross_attentions\n",
    "        if hasattr(outputs, \"cross_attentions\") and outputs.cross_attentions\n",
    "        else outputs.decoder_attentions\n",
    "    )\n",
    "\n",
    "    if return_mdi:\n",
    "        mdi = compute_mdi(attns)\n",
    "        if return_attn:\n",
    "            return answer, mdi, attns\n",
    "        return answer, mdi\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e99780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "# ---- 1. Load config and enable attentions ----\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.output_attentions = True\n",
    "config.return_dict = True\n",
    "config.return_dict_in_generate = True\n",
    "\n",
    "# ---- 2. Load processor normally ----\n",
    "print(\"Loading BLIP model...\")\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ---- 3. Load model with modified config ----\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (with attentions enabled)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. \n",
    "You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION is NOT ground truth. It was only context shown to another model.\n",
    "  Do NOT trust it, and do NOT use it to judge correctness.\n",
    "• Ignore any misleading, missing, or incorrect details in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only, NOT truth): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b50475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2\n",
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_BLIP_outputs(subset_size=None, last_turn_only=False):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(BLIP_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "#                 incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "                L4 = q[\"L4\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                \n",
    "                history_correct = [] \n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3), (\"L4\", L4)]:\n",
    "                    ans, mdi, attn = ask_blip2(path, correct_caption, q, history=history_correct, return_mdi=True, return_attn=True, last_turn_only=last_turn_only)\n",
    "                    \n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "\n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    # entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    # Single caption for this experiment\n",
    "                    \"caption\": correct_caption,\n",
    "\n",
    "                    # Questions per difficulty level\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "                        \"L4\": L4\n",
    "                    },\n",
    "\n",
    "                    # Model answers (BLIP/LLaVA/etc.)\n",
    "                    \"answers\": answers_correct,   # dict: {\"L0\": ..., \"L1\": ..., ...}\n",
    "\n",
    "                    # Metrics grouped by level\n",
    "                    \"metrics\": {},                # will fill below\n",
    "\n",
    "                    # Claude eval scores for each level\n",
    "                    \"eval_scores\": {}             # will fill later\n",
    "                }\n",
    "        \n",
    "                levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "\n",
    "                for lvl in levels:\n",
    "                    output[\"metrics\"][lvl] = {\n",
    "                    \"mdi\": mdi_correct.get(lvl),\n",
    "                    \"entropy\": entropy_correct.get(lvl),\n",
    "                    \"shift\": shift_correct.get(lvl)\n",
    "                }\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"caption\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    output[\"eval_scores\"][level] = score_c\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {BLIP_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-modal comparison (will be implemented in another file)\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def jsonl_to_df(jsonl_path):\n",
    "#     \"\"\"\n",
    "#     Reads your blip_responses.jsonl or llava_responses.jsonl file\n",
    "#     and converts it into a clean pandas DataFrame.\n",
    "    \n",
    "#     Handles:\n",
    "#     - JSONL line-by-line format\n",
    "#     - correct/incorrect caption scores\n",
    "#     - nested mdi/entropy/shift dicts\n",
    "#     - missing/null values safely\n",
    "#     \"\"\"\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     with open(jsonl_path, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "\n",
    "#             # Load current JSON object (one per line)\n",
    "#             entry = json.loads(line)\n",
    "\n",
    "#             image_id = entry[\"image_id\"]\n",
    "\n",
    "#             # Loop over correct_caption / incorrect_caption\n",
    "#             for caption_type in [\"correct_caption\", \"incorrect_caption\"]:\n",
    "\n",
    "#                 # Loop over L0/L1/L2/L3 levels\n",
    "#                 for lvl in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "\n",
    "#                     # Safely extract nested fields with .get()\n",
    "#                     mdi_val = entry[\"mdi_scores\"][caption_type].get(lvl)\n",
    "#                     entropy_val = entry[\"entropy_scores\"][caption_type].get(lvl)\n",
    "#                     shift_val = entry[\"shift_scores\"][caption_type].get(lvl)\n",
    "\n",
    "#                     # Evaluation: 0 or 1\n",
    "#                     correct_val = entry[\"eval_scores\"][caption_type].get(\n",
    "#                         f\"{caption_type}_score\"\n",
    "#                     )\n",
    "\n",
    "#                     row = {\n",
    "#                         \"image_id\": image_id,\n",
    "#                         \"caption_type\": caption_type,\n",
    "#                         \"level\": lvl,\n",
    "#                         \"mdi\": mdi_val,\n",
    "#                         \"entropy\": entropy_val,\n",
    "#                         \"shift\": shift_val,\n",
    "#                         \"correct\": correct_val,\n",
    "#                     }\n",
    "\n",
    "#                     rows.append(row)\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     # Convert None to NaN (cleaner for plotting)\n",
    "#     df = df.replace({None: np.nan})\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea721b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ######### BLIP #########\n",
    "    \n",
    "    #Generates dataset used (correct/incorrect captions, L0-L4 questions)\n",
    "    #Evaluates BLIP-2 responses via Claude Sonnet 4.5 (0 - incorrect; 1 - correct)\n",
    "    generate_BLIP_outputs(subset_size=5, last_turn_only=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
