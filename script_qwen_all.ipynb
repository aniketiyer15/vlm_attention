{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\"\n",
    "GEMINI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "\n",
    "QWEN_EXP1_SINGLE_OUTPUT_PATH = \"qwen_exp1_single_responses.jsonl\" \n",
    "QWEN_EXP2_SINGLE_OUTPUT_PATH = \"qwen_exp2_single_responses.jsonl\" \n",
    "\n",
    "QWEN_EXP1_MULTI_OUTPUT_PATH = \"qwen_exp1_multi_responses.jsonl\" \n",
    "QWEN_EXP2_MULTI_OUTPUT_PATH = \"qwen_exp2_multi_responses.jsonl\" \n",
    "\n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "GOOGLE_MODEL = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a18fc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44351eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_exp1(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "• Must accurately describe the visible scene.\n",
    "• 7–15 words, objective, simple, and factual.\n",
    "• Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "• Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "• Must keep the same length and sentence structure style as the correct caption.\n",
    "• MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     – species/category of the main object\n",
    "     – color of a main object\n",
    "     – pattern/texture of a main object\n",
    "     – object type that a person is holding/using\n",
    "     – action the main subject is performing\n",
    "     – spatial relation (e.g., “in front of” → “behind”)\n",
    "     \n",
    "• The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, “ocean” is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "• The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "• The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0–L4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 — Pure language prior  \n",
    "• Must be answerable with NO access to the image.  \n",
    "• General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "• 6–14 words.\n",
    "\n",
    "L1 — Probe changed attribute #1 \n",
    "• MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "• Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "• Example:If species changed, ask “What type of animal…?”  \n",
    "          If color changed, ask “What color is…?”  \n",
    "          If object type changed, ask “What object is… holding?”  \n",
    "• No attributes other than the first changed one.  \n",
    "• 6–14 words.\n",
    "\n",
    "L2 — Probe changed attribute #2\n",
    "• MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "• Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "• Same rules as L2 but targeting the second changed detail.  \n",
    "• Should not be the same question as L1. \n",
    "• 6–14 words.\n",
    "\n",
    "L3 — High-level reasoning\n",
    "• Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "• The question MUST NOT depend on the two changed attributes.\n",
    "• The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "• The question SHOULD require general common-sense or contextual reasoning.\n",
    "• The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "• 6–14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "• Do NOT provide answers.\n",
    "• Do NOT describe the image outside captions.\n",
    "• All questions must be 6–14 words.\n",
    "• Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_exp2(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "============================================================\n",
    "1. Correct Caption\n",
    "============================================================\n",
    "• Accurately describe the visible scene.\n",
    "• 9–15 words, objective, simple, and factual.\n",
    "• Should mention main objects; avoid inference beyond evidence.\n",
    "\n",
    "============================================================\n",
    "2. Visual Necessity Question Ladder (VNL): Levels L0 → L4\n",
    "============================================================\n",
    "\n",
    "GENERAL RULES:\n",
    "• L1–L4 MUST require looking at the image to answer.\n",
    "• All questions MUST be answerable using only the given image.\n",
    "• Do NOT include the answers.\n",
    "• No question should exceed 14 words.\n",
    "• Return concise, natural wording.\n",
    "\n",
    "------------------------------------------------------------\n",
    "L0 – Baseline Question (Language-prior only)\n",
    "------------------------------------------------------------\n",
    "• A question humans can answer **without seeing the image**.\n",
    "• May refer to the world generally (NOT the specific image).\n",
    "• Purpose: control for language-only biases.\n",
    "• 6–12 words.\n",
    "Examples:\n",
    "– “What season often has the coldest weather?”  \n",
    "– “Which animal is larger, a dog or an elephant?”  \n",
    "– “What do people usually use to take photographs?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L1 – Basic Visual Recognition\n",
    "------------------------------------------------------------\n",
    "• Requires the image.\n",
    "• Ask about a **primary object** or its basic property.\n",
    "• No reasoning, no inference.\n",
    "Examples:\n",
    "– “What object is the person holding?”  \n",
    "– “What color is the animal?”  \n",
    "– “How many people are visible?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L2 – Intermediate Visual Detail\n",
    "------------------------------------------------------------\n",
    "• Also requires the image.\n",
    "• Ask about a **secondary property** of a main object.\n",
    "• Slightly more specific than L1.\n",
    "Examples:\n",
    "– “What pattern is on the person’s shirt?”  \n",
    "– “What type of hat is the man wearing?”  \n",
    "– “What material is the table made of?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L3 – Relational / Spatial Reasoning\n",
    "------------------------------------------------------------\n",
    "• Requires image + spatial relations + relational understanding.\n",
    "Examples:\n",
    "– “Where is the dog positioned relative to the child?”  \n",
    "– “What object is behind the bicycle?”  \n",
    "– “Which person is closest to the camera?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L4 – High-Level Visual Reasoning\n",
    "------------------------------------------------------------\n",
    "• Hardest level; requires the entire scene.\n",
    "• Ask about interactions, goals, implied roles, or multi-object context.\n",
    "• Still must be answerable from the image alone (no external inference).\n",
    "Examples:\n",
    "– “What activity are the people engaged in?”  \n",
    "– “Why is the man extending his arm?”  \n",
    "– “What is the group collectively doing?”\n",
    "\n",
    "============================================================\n",
    "Return EXACTLY this JSON structure:\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",\n",
    "  \"L2\": \"<string>\",\n",
    "  \"L3\": \"<string>\",\n",
    "  \"L4\": \"<string>\"\n",
    "}\n",
    "============================================================\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219955a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WORKING - but may be wrong!\n",
    "\n",
    "# def compute_qwen_mdi(attns, inputs, image_token_id=151655):\n",
    "#     \"\"\"\n",
    "#     MDI for a single-image Qwen2.5-VL call.\n",
    "\n",
    "#     attns  : list of attention tensors captured from decoder layers\n",
    "#              each with shape (batch, heads, q_len, k_len)\n",
    "#              (we'll gracefully skip anything that isn't this)\n",
    "#     inputs : batch dict that contains \"input_ids\"\n",
    "#     \"\"\"\n",
    "\n",
    "#     import torch\n",
    "\n",
    "#     if attns is None or len(attns) == 0:\n",
    "#         print(\"MDI: no attention tensors\")\n",
    "#         return None\n",
    "\n",
    "#     if \"input_ids\" not in inputs:\n",
    "#         print(\"MDI: inputs missing input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     input_ids = inputs[\"input_ids\"][0]          # (seq_len,)\n",
    "#     img_positions = (input_ids == image_token_id).nonzero().flatten()\n",
    "\n",
    "#     if img_positions.numel() == 0:\n",
    "#         print(\"MDI: no image tokens found in input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     img_start = img_positions[0].item()\n",
    "#     img_end   = img_positions[-1].item() + 1    # non-inclusive\n",
    "\n",
    "#     vision_scores = []\n",
    "#     text_scores = []\n",
    "\n",
    "#     for layer_attn in attns:\n",
    "#         # Some layers may return None or have wrong shape; skip them\n",
    "#         if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "#             continue\n",
    "#         if layer_attn.dim() != 4:\n",
    "#             # e.g. (heads, q, k) -> add batch dim\n",
    "#             if layer_attn.dim() == 3:\n",
    "#                 layer_attn = layer_attn.unsqueeze(0)\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#         # layer_attn: (batch, heads, q_len, k_len)\n",
    "#         attn = layer_attn.mean(dim=1)[0]  # -> (q_len, k_len)\n",
    "\n",
    "#         # safety in case sequence length changed\n",
    "#         q_len, k_len = attn.shape\n",
    "#         if img_end > k_len:\n",
    "#             continue\n",
    "\n",
    "#         vis = attn[:, img_start:img_end].sum().item()\n",
    "#         txt = (attn[:, :img_start].sum() + attn[:, img_end:].sum()).item()\n",
    "\n",
    "#         vision_scores.append(vis)\n",
    "#         text_scores.append(txt)\n",
    "\n",
    "#     if not vision_scores or not text_scores:\n",
    "#         print(\"MDI: no valid layers after filtering\")\n",
    "#         return None\n",
    "\n",
    "#     vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "#     txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "#     mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "#     return float(mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942f19c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WORKING - but may be wrong!\n",
    "\n",
    "# def compute_qwen_mdi(attns, inputs):\n",
    "#     \"\"\"\n",
    "#     Guaranteed-working MDI for Qwen2.5-VL.\n",
    "#     Automatically detects visual tokens using the known Qwen ranges:\n",
    "#       - 151552–151654 : image codebook tokens\n",
    "#       - 151655        : image separator/end token\n",
    "\n",
    "#     MDI = attention_to_visual_tokens / (attention_to_all_other_tokens)\n",
    "#     \"\"\"\n",
    "\n",
    "#     import torch\n",
    "\n",
    "#     if attns is None or len(attns) == 0:\n",
    "#         print(\"MDI: no attn tensors\")\n",
    "#         return None\n",
    "\n",
    "#     if \"input_ids\" not in inputs:\n",
    "#         print(\"MDI: missing input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     input_ids = inputs[\"input_ids\"][0]           # (seq_len,)\n",
    "\n",
    "#     # --- 1. Detect vision tokens robustly ---\n",
    "#     vision_mask = ((input_ids >= 151552) & (input_ids <= 151655))\n",
    "\n",
    "#     visual_positions = vision_mask.nonzero().flatten()\n",
    "#     if visual_positions.numel() == 0:\n",
    "#         print(\"MDI: no visual tokens detected\")\n",
    "#         return None\n",
    "\n",
    "#     v_start = visual_positions[0].item()\n",
    "#     v_end   = visual_positions[-1].item() + 1    # non-inclusive\n",
    "\n",
    "#     vision_scores = []\n",
    "#     text_scores   = []\n",
    "\n",
    "#     # --- 2. Iterate over captured attention ---\n",
    "#     for layer_attn in attns:\n",
    "\n",
    "#         if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "#             continue\n",
    "\n",
    "#         # Accept shapes:\n",
    "#         #   (batch, heads, q_len, k_len)\n",
    "#         #   (heads, q_len, k_len)\n",
    "#         if layer_attn.dim() == 3:\n",
    "#             # add batch dim\n",
    "#             layer_attn = layer_attn.unsqueeze(0)\n",
    "#         elif layer_attn.dim() != 4:\n",
    "#             continue\n",
    "\n",
    "#         # Mean over heads -> (q_len, k_len)\n",
    "#         attn = layer_attn.mean(dim=1)[0]\n",
    "\n",
    "#         q_len, k_len = attn.shape\n",
    "\n",
    "#         # clip if KV cache truncated\n",
    "#         if v_end > k_len:\n",
    "#             continue\n",
    "\n",
    "#         # total attention paid to image tokens\n",
    "#         vis = attn[:, v_start:v_end].sum().item()\n",
    "\n",
    "#         # attention paid to everything else\n",
    "# #         txt = (attn[:, :v_start].sum() + attn[:, v_end:].sum()).item()\n",
    "\n",
    "#         txt_before = attn[:, :v_start].sum().item() if v_start > 0 else 0\n",
    "#         txt_after = attn[:, v_end:].sum().item() if v_end < k_len else 0\n",
    "#         txt = txt_before + txt_after\n",
    "\n",
    "#         vision_scores.append(vis)\n",
    "#         text_scores.append(txt)\n",
    "\n",
    "#     if len(vision_scores) == 0 or len(text_scores) == 0:\n",
    "#         print(\"MDI: no valid layers\")\n",
    "#         return None\n",
    "\n",
    "#     vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "#     txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "#     mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "#     return float(mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ead11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working - most likely correct!\n",
    "\n",
    "def compute_qwen_mdi(attns, inputs):\n",
    "    \"\"\"\n",
    "    Guaranteed-correct MDI for Qwen2.5-VL.\n",
    "    Qwen does NOT place real vision tokens in input_ids.\n",
    "    It only inserts repeated <image> placeholder tokens with ID 151655.\n",
    "    The actual image patches stay inside the vision encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    import torch\n",
    "\n",
    "    if attns is None or len(attns) == 0:\n",
    "        return None\n",
    "\n",
    "    if \"input_ids\" not in inputs:\n",
    "        return None\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0]          # (seq_len,)\n",
    "    seq_len = input_ids.shape[0]\n",
    "\n",
    "    # ---- 1. Correct visual span detection ----\n",
    "    # Qwen2.5-VL uses ONLY token_id 151655 as the image placeholder\n",
    "    IMAGE_TOKEN_ID = 151655\n",
    "\n",
    "    visual_positions = (input_ids == IMAGE_TOKEN_ID).nonzero().flatten()\n",
    "    if visual_positions.numel() == 0:\n",
    "        print(\"No 151655 tokens found → Qwen image token missing?\")\n",
    "        return None\n",
    "\n",
    "    v_start = visual_positions[0].item()\n",
    "    v_end   = visual_positions[-1].item() + 1     # non-inclusive\n",
    "\n",
    "    # ---- 2. Accumulate attention ----\n",
    "    vision_scores = []\n",
    "    text_scores = []\n",
    "\n",
    "    for layer_attn in attns:\n",
    "\n",
    "        if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "            continue\n",
    "\n",
    "        # expected (batch, heads, q_len, k_len)\n",
    "        if layer_attn.dim() == 3:\n",
    "            layer_attn = layer_attn.unsqueeze(0)\n",
    "        elif layer_attn.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        attn = layer_attn.mean(dim=1)[0]    # (q_len, k_len)\n",
    "\n",
    "        q_len, k_len = attn.shape\n",
    "        if v_end > k_len:\n",
    "            continue\n",
    "\n",
    "        # vision attention\n",
    "        vis = attn[:, v_start:v_end].sum().item()\n",
    "\n",
    "        # text attention\n",
    "        txt_before = attn[:, :v_start].sum().item()\n",
    "        txt_after  = attn[:, v_end:].sum().item()\n",
    "        txt = txt_before + txt_after\n",
    "\n",
    "        vision_scores.append(vis)\n",
    "        text_scores.append(txt)\n",
    "\n",
    "    if len(vision_scores) == 0:\n",
    "        return None\n",
    "\n",
    "    vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "    txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "    mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "    return float(mdi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550715e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# NOT NORMALIZED \n",
    "\n",
    "def compute_attention_entropy(attns):\n",
    "    \"\"\"\n",
    "    Compute average attention entropy across all Qwen2-VL decoder layers.\n",
    "    \n",
    "    attns : list of attention tensors captured by hooks\n",
    "            each element is (batch, heads, q_len, k_len) or (heads, q_len, k_len)\n",
    "\n",
    "    Returns:\n",
    "        float entropy_score  (lower = more focused, higher = more diffuse)\n",
    "        or None if not computable\n",
    "    \"\"\"\n",
    "\n",
    "    if attns is None or len(attns) == 0:\n",
    "        print(\"Entropy: no attention tensors\")\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in attns:\n",
    "\n",
    "        # Skip invalid entries\n",
    "        if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "            continue\n",
    "\n",
    "        # Ensure shape is (batch, heads, q, k)\n",
    "        if layer_attn.dim() == 3:\n",
    "            layer_attn = layer_attn.unsqueeze(0)   # (1, heads, q, k)\n",
    "        elif layer_attn.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        # Normalize attention along key dimension\n",
    "        # shape: (batch, heads, q_len, k_len)\n",
    "        attn = layer_attn.float()\n",
    "\n",
    "        # Softmax normalization (just in case the model didn't return normalized attn)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Compute entropy per (batch, head, q)\n",
    "        # H = -sum(p * log(p))\n",
    "        entropy = -(attn * (attn + 1e-12).log()).sum(dim=-1)  # sum over k_len\n",
    "\n",
    "        # Mean over batch, heads, and q positions\n",
    "        entropy = entropy.mean().item()\n",
    "\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    if len(entropies) == 0:\n",
    "        print(\"Entropy: no valid layers\")\n",
    "        return None\n",
    "\n",
    "    # Average entropy across layers\n",
    "    final_entropy = float(sum(entropies) / len(entropies))\n",
    "    return final_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae33f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Version w/ normalization (scaled from 0 to 1)\n",
    "\n",
    "def compute_attention_entropy(attn_maps, mask=None, vision_span=None, normalized=True):\n",
    "    \"\"\"\n",
    "    attn_maps : list of attention tensors\n",
    "        each tensor has shape (batch, heads, q_len, k_len)\n",
    "        or (heads, q_len, k_len)\n",
    "        or (q_len, k_len)\n",
    "\n",
    "    mask : optional boolean mask of shape (k_len,)\n",
    "        True = include that key token\n",
    "        If None, full sequence is used.\n",
    "\n",
    "    vision_span : optional (start, end)\n",
    "        If provided, computes entropy only over this token region.\n",
    "\n",
    "    normalized : bool\n",
    "        If True -> returns H / log(k)\n",
    "        If False -> returns raw entropy.\n",
    "\n",
    "    Returns:\n",
    "        average entropy across layers + heads as float\n",
    "    \"\"\"\n",
    "    if attn_maps is None or len(attn_maps) == 0:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in attn_maps:\n",
    "\n",
    "        # ---- 1. Fix shapes ----\n",
    "        if layer_attn.dim() == 3:       # (heads, q, k)\n",
    "            layer_attn = layer_attn.unsqueeze(0)  # -> (1, heads, q, k)\n",
    "        elif layer_attn.dim() == 2:     # (q, k)\n",
    "            layer_attn = layer_attn.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        if layer_attn.dim() != 4:\n",
    "            continue  # bad layer\n",
    "\n",
    "        b, h, q_len, k_len = layer_attn.shape\n",
    "\n",
    "        # ---- 2. Select region (vision / mask) ----\n",
    "        attn = layer_attn  # (b, heads, q, k)\n",
    "\n",
    "        if vision_span is not None:\n",
    "            start, end = vision_span\n",
    "            attn = attn[:, :, :, start:end]\n",
    "            k_len = end - start\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: (k,)\n",
    "            mask = mask.to(attn.device)\n",
    "            attn = attn[:, :, :, mask]\n",
    "            k_len = mask.sum().item()\n",
    "\n",
    "        if k_len <= 1:\n",
    "            continue\n",
    "\n",
    "        # ---- 3. Normalize probabilities (safety) ----\n",
    "        attn = attn.clamp(min=1e-9)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # ---- 4. Entropy ----\n",
    "        H = -(attn * attn.log()).sum(dim=-1)  # (b, heads, q)\n",
    "        H = H.mean().item()  # average everything\n",
    "\n",
    "        # ---- 5. Normalize 0–1 if requested ----\n",
    "        if normalized:\n",
    "            H = H / math.log(k_len)\n",
    "\n",
    "        entropies.append(H)\n",
    "\n",
    "    if len(entropies) == 0:\n",
    "        return None\n",
    "\n",
    "    return float(sum(entropies) / len(entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56738872",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WRONG - but keeping just in case\n",
    "\n",
    "# def compute_attention_shift(attns_A, attns_B, vision_span=None, normalized=True):\n",
    "#     \"\"\"\n",
    "#     Computes attention shift between two model runs (A and B).\n",
    "    \n",
    "#     Each attns_X is a list of attention tensors:\n",
    "#        (batch, heads, q_len, k_len) or (heads, q_len, k_len) or (q_len, k_len).\n",
    "\n",
    "#     vision_span: (start, end)  → compute shift only over visual tokens\n",
    "#     normalized: normalize final shift to [0, 1]\n",
    "\n",
    "#     Returns:\n",
    "#         scalar attention-shift score\n",
    "#     \"\"\"\n",
    "\n",
    "#     if attns_A is None or attns_B is None:\n",
    "#         print(\"Shift: missing attn maps\")\n",
    "#         return None\n",
    "\n",
    "#     if len(attns_A) == 0 or len(attns_B) == 0:\n",
    "#         print(\"Shift: empty attn lists\")\n",
    "#         return None\n",
    "\n",
    "#     shifts = []\n",
    "\n",
    "#     # Iterate layer-by-layer (stop at min length)\n",
    "#     L = min(len(attns_A), len(attns_B))\n",
    "\n",
    "#     for i in range(L):\n",
    "#         A = attns_A[i]\n",
    "#         B = attns_B[i]\n",
    "\n",
    "#         # Ensure both are valid tensors\n",
    "#         if A is None or B is None:\n",
    "#             continue\n",
    "#         if not (torch.is_tensor(A) and torch.is_tensor(B)):\n",
    "#             continue\n",
    "\n",
    "#         # ---- 1. Normalize shapes to (1, heads, q, k) ----\n",
    "#         def fix_shape(X):\n",
    "#             if X.dim() == 2:      # (q, k)\n",
    "#                 return X.unsqueeze(0).unsqueeze(0)\n",
    "#             if X.dim() == 3:      # (heads, q, k)\n",
    "#                 return X.unsqueeze(0)\n",
    "#             return X  # assume (batch, heads, q, k)\n",
    "        \n",
    "#         A = fix_shape(A)\n",
    "#         B = fix_shape(B)\n",
    "\n",
    "#         # ---- 2. Align sequence lengths ----\n",
    "#         _, hA, qA, kA = A.shape\n",
    "#         _, hB, qB, kB = B.shape\n",
    "\n",
    "#         q = min(qA, qB)\n",
    "#         k = min(kA, kB)\n",
    "\n",
    "#         A = A[:, :, :q, :k]\n",
    "#         B = B[:, :, :q, :k]\n",
    "\n",
    "#         # ---- 3. Optional: restrict to vision tokens ----\n",
    "#         if vision_span is not None:\n",
    "#             v_start, v_end = vision_span\n",
    "#             v_end = min(v_end, k)\n",
    "#             A = A[:, :, :, v_start:v_end]\n",
    "#             B = B[:, :, :, v_start:v_end]\n",
    "\n",
    "#         # ---- 4. Normalize to probability distributions ----\n",
    "#         A = A.clamp(min=1e-9)\n",
    "#         B = B.clamp(min=1e-9)\n",
    "\n",
    "#         A = A / A.sum(dim=-1, keepdim=True)\n",
    "#         B = B / B.sum(dim=-1, keepdim=True)\n",
    "\n",
    "#         # ---- 5. L1 distance (attention shift) ----\n",
    "#         # average across batch, heads, and queries\n",
    "#         shift_val = torch.abs(A - B).sum(dim=-1).mean().item()\n",
    "\n",
    "#         shifts.append(shift_val)\n",
    "\n",
    "#     if len(shifts) == 0:\n",
    "#         print(\"Shift: no valid layers after processing\")\n",
    "#         return None\n",
    "\n",
    "#     shift_raw = sum(shifts) / len(shifts)\n",
    "\n",
    "#     # ---- 6. Normalize shift to 0–1 scale ----\n",
    "#     # Maximum L1 between two distributions is 2\n",
    "#     if normalized:\n",
    "#         shift_raw = shift_raw / 2.0  \n",
    "\n",
    "#     return float(shift_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c71cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention_shift(prev_attns, curr_attns, inputs):\n",
    "    \"\"\"\n",
    "    Computes attention shift between two levels for Qwen2.5-VL.\n",
    "    Uses only LAST COMMON QUERY token to avoid shape mismatch.\n",
    "    \"\"\"\n",
    "\n",
    "    if prev_attns is None or curr_attns is None:\n",
    "        return None\n",
    "    if len(prev_attns) == 0 or len(curr_attns) == 0:\n",
    "        return None\n",
    "\n",
    "    # ===== 1. Find visual region using Qwen input_ids =====\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    visual_mask = ((input_ids >= 151552) & (input_ids <= 151655))\n",
    "    visual_positions = visual_mask.nonzero().flatten()\n",
    "\n",
    "    if visual_positions.numel() == 0:\n",
    "        print(\"Shift: no visual tokens detected in input_ids\")\n",
    "        return None\n",
    "\n",
    "    v_start = visual_positions[0].item()\n",
    "    v_end   = visual_positions[-1].item() + 1\n",
    "\n",
    "    layer_shifts = []\n",
    "\n",
    "    # ===== 2. Layer-by-layer shift =====\n",
    "    for A_prev, A_curr in zip(prev_attns, curr_attns):\n",
    "\n",
    "        if A_prev is None or A_curr is None:\n",
    "            continue\n",
    "        if not (torch.is_tensor(A_prev) and torch.is_tensor(A_curr)):\n",
    "            continue\n",
    "\n",
    "        # unify shapes\n",
    "        if A_prev.dim() == 3: A_prev = A_prev.unsqueeze(0)\n",
    "        if A_curr.dim() == 3: A_curr = A_curr.unsqueeze(0)\n",
    "        if A_prev.dim() != 4 or A_curr.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        # mean over heads → (q_len, k_len)\n",
    "        A_prev = A_prev.mean(dim=1)[0]\n",
    "        A_curr = A_curr.mean(dim=1)[0]\n",
    "\n",
    "        q_len_prev, k_len_prev = A_prev.shape\n",
    "        q_len_curr, k_len_curr = A_curr.shape\n",
    "\n",
    "        # align key dimension\n",
    "        k_len = min(k_len_prev, k_len_curr)\n",
    "        A_prev = A_prev[:, :k_len]\n",
    "        A_curr = A_curr[:, :k_len]\n",
    "\n",
    "        # visual slice must be valid\n",
    "        if v_end > k_len:\n",
    "            continue\n",
    "\n",
    "        # ===== *** FIX: align QUERY dimension *** =====\n",
    "        q_len = min(q_len_prev, q_len_curr)\n",
    "        # pick last common query\n",
    "        A_prev_last = A_prev[q_len - 1, v_start:v_end]\n",
    "        A_curr_last = A_curr[q_len - 1, v_start:v_end]\n",
    "\n",
    "        # normalize\n",
    "        A_prev_last = A_prev_last.clamp(1e-9)\n",
    "        A_curr_last = A_curr_last.clamp(1e-9)\n",
    "        A_prev_last = A_prev_last / A_prev_last.sum()\n",
    "        A_curr_last = A_curr_last / A_curr_last.sum()\n",
    "\n",
    "        # cosine distance\n",
    "        cos_sim = F.cosine_similarity(\n",
    "            A_prev_last, A_curr_last, dim=0\n",
    "        )\n",
    "        shift = float(1 - cos_sim.clamp(-1, 1))\n",
    "        layer_shifts.append(shift)\n",
    "\n",
    "    if len(layer_shifts) == 0:\n",
    "        return None\n",
    "\n",
    "    return sum(layer_shifts) / len(layer_shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87fb8e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ask_qwen(\n",
    "    image_path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_metrics=True,\n",
    "    last_turn_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs Qwen-VL with image + (caption + question) text prompt.\n",
    "    Supports:\n",
    "        - returning answer only\n",
    "        - returning answer + MDI\n",
    "        - returning answer + MDI + attention tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 0. Initialize ----------\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # ---------- 1. Load image ----------\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---------- 2. Build fixed-format prompt ----------\n",
    "    answer_rules = (\n",
    "        \"Please answer in plain text only.\\n\"\n",
    "        \"Do NOT use markdown formatting.\\n\"\n",
    "        \"Keep the answer short (1–2 sentences).\\n\"\n",
    "        \"Provide only the direct answer without any explanation.\"\n",
    "    )\n",
    "\n",
    "    # ---------- 3. Build message list in the CORRECT, SAFE ORDER ----------\n",
    "    messages = []\n",
    "\n",
    "    # (A) Add history FIRST (chronological)\n",
    "    for q_prev, a_prev in history:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": q_prev}\n",
    "            ]\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": a_prev}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # (B) Add CURRENT TURN LAST\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # 1. QUESTION FIRST — prevents L0 contamination\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "\n",
    "            # 2. IMAGE SECOND\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "\n",
    "            # 3. CAPTION THIRD — labeled to avoid mixing with instructions\n",
    "            {\"type\": \"text\", \"text\": f\"Caption: {caption}\"},\n",
    "\n",
    "            # 4. ANSWER RULES LAST — separate block\n",
    "            {\"type\": \"text\", \"text\": answer_rules}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # ---------- 4. Preprocess ----------\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # ---------- 5. Vision preprocess ----------\n",
    "    try:\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "    except ImportError:\n",
    "        # fallback\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "    # IMPORTANT for attention extraction\n",
    "    model.config.use_cache = False\n",
    "\n",
    "#     # ---------- 6. Generate with attention extraction ----------\n",
    "    \n",
    "    \n",
    "    def find_decoder_self_attn_layers(model):\n",
    "        layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if name.endswith(\"self_attn\") and \"language_model\" in name:\n",
    "                layers.append(module)\n",
    "        return layers\n",
    "    \n",
    "    collected_attns = []\n",
    "\n",
    "    def save_attn(module, inp, out):\n",
    "        attn = out[1]  # (batch, heads, q_len, k_len)\n",
    "        collected_attns.append(attn.detach().cpu())\n",
    "\n",
    "    layers = find_decoder_self_attn_layers(model)\n",
    "    \n",
    "    hooks = []\n",
    "    for layer in layers:\n",
    "        h = layer.register_forward_hook(save_attn)\n",
    "        hooks.append(h)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        \n",
    "    # --- REMOVE HOOKS ---\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "\n",
    "    # ---------- 7. Decode answer ----------\n",
    "    generated_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]  # remove prompt tokens\n",
    "    answer = processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Remove possible \"ASSISTANT:\" prefixes\n",
    "    if \"ASSISTANT:\" in answer:\n",
    "        answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "    # ---------- 8. Metrics ----------\n",
    "    if return_metrics:\n",
    "        try:\n",
    "            final_mdi = compute_qwen_mdi(collected_attns, inputs)\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "        except:\n",
    "            print(\"fail\")\n",
    "            final_mdi = None\n",
    "\n",
    "        return answer, final_mdi, collected_attns, inputs # INPUTS ADDED FOR ATTENTION SHIFT CALC\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47596a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QWEN MODEL\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "print(\"Loading Qwen model...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "print(\"✅ Qwen Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfe39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "#     \"\"\"\n",
    "#     Uses Google Gemini to judge the answer.\n",
    "#     Returns 0 or 1.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Load the image using PIL (Gemini likes this format)\n",
    "#     img = Image.open(image_path)\n",
    "    \n",
    "#     # 2. Get your prompt text\n",
    "#     # (Keep your create_eval_prompt function exactly the same as it is now)\n",
    "#     prompt_text = create_eval_prompt(caption, condition, question, model_answer)\n",
    "    \n",
    "#     try:\n",
    "#         # 3. Call Gemini\n",
    "#         # We pass the text prompt AND the image object in a list\n",
    "#         response = GOOGLE_MODEL.generate_content([prompt_text, img])\n",
    "        \n",
    "#         # 4. Clean the output\n",
    "#         output = response.text.strip()\n",
    "        \n",
    "#         # Gemini might be chatty, ensure we just get the number\n",
    "#         if \"1\" in output: return 1\n",
    "#         if \"0\" in output: return 0\n",
    "        \n",
    "#         # Fallback if it returns something else\n",
    "#         return 0\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Gemini Error: {e}\")\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d48c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to setup eval metric calculation\n",
    "\n",
    "def pair_stats_by_level(jsonl_path):\n",
    "    levels = [\"L0\", \"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "    # Tallies per level\n",
    "    pair_stats = {\n",
    "        lvl: {(1,1):0, (1,0):0, (0,1):0, (0,0):0}\n",
    "        for lvl in levels\n",
    "    }\n",
    "\n",
    "    # ---- Single JSONL pass ----\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            for lvl in levels:\n",
    "                s_c = eval_scores[lvl][\"correct_caption_score\"]\n",
    "                s_i = eval_scores[lvl][\"incorrect_caption_score\"]\n",
    "                pair_stats[lvl][(s_c, s_i)] += 1\n",
    "\n",
    "    return pair_stats\n",
    "\n",
    "def conf_pairs_by_level(pair_stats):\n",
    "    return pair_stats  # already exactly the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d774c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fooling_rate_by_level(pair_stats):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c10 = counts[(1,0)]\n",
    "        total = sum(counts.values())\n",
    "        rate = c10 / total if total > 0 else 0\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"fooled\": c10,\n",
    "            \"total\": total,\n",
    "            \"rate\": rate,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metrics - per-level answer accuracy, computed separately for the correct-caption and incorrect-caption conditions.\n",
    "\n",
    "def acc_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c11 = counts[(1,1)]\n",
    "        c10 = counts[(1,0)]\n",
    "        c01 = counts[(0,1)]\n",
    "        c00 = counts[(0,0)]\n",
    "        total = c11 + c10 + c01 + c00\n",
    "\n",
    "        if total == 0:\n",
    "            results[lvl] = None\n",
    "            continue\n",
    "\n",
    "        # accuracy under correct caption = model is correct (regardless of incorrect-caption score)\n",
    "        acc_correct = (c11 + c10) / total\n",
    "        # accuracy under incorrect caption = model is correct under wrong caption\n",
    "        acc_incorrect = (c11 + c01) / total\n",
    "\n",
    "#         mdi = acc_correct - acc_incorrect\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"accuracy_correct_caption\": acc_correct,\n",
    "            \"accuracy_incorrect_caption\": acc_incorrect\n",
    "#             \"MDI\": mdi,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb107e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_qwen_outputs_exp1_single(subset_size=None):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(QWEN_EXP1_SINGLE_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions_exp1(b64)\n",
    "                \n",
    "                # ---- CLEAN CAPTIONS HERE ----\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "#                 history_correct = []\n",
    "\n",
    "                prev_attn = None\n",
    "        \n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, correct_caption, q, return_metrics=True)\n",
    "                                \n",
    "#                     if last_turn_only: \n",
    "#                         history_correct = [(q, ans)]\n",
    "#                     else:\n",
    "#                         history_correct.append((q, ans))\n",
    "                        \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    #entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "                answers_incorrect = {}\n",
    "                mdi_incorrect = {}\n",
    "                entropy_incorrect = {}\n",
    "                shift_incorrect = {}\n",
    "#                 history_incorrect = []\n",
    "\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, incorrect_caption, q, return_metrics=True)\n",
    "\n",
    "                    answers_incorrect[lvl] = ans\n",
    "                    mdi_incorrect[lvl] = round(mdi, 3)\n",
    "        \n",
    "#                     if last_turn_only: \n",
    "#                         history_correct = [(q, ans)]\n",
    "#                     else:\n",
    "#                         history_correct.append((q, ans))\n",
    "                        \n",
    "\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_incorrect[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    if prev_attn is None:\n",
    "                        shift_incorrect[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_incorrect[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    }, \n",
    "                    \n",
    "                    \"mdi_scores\": { \n",
    "                        \"correct_caption\": mdi_correct,     \n",
    "                        \"incorrect_caption\": mdi_incorrect \n",
    "                    },\n",
    "    \n",
    "                    \"entropy_scores\": {\n",
    "                        \"correct_caption\": entropy_correct,\n",
    "                        \"incorrect_caption\": entropy_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"shift_scores\": {\n",
    "                        \"correct_caption\": shift_correct,\n",
    "                        \"incorrect_caption\": shift_incorrect\n",
    "                    },\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {QWEN_EXP1_SINGLE_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676611c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# def create_random_subset(subset_size, subset_name=None):\n",
    "#     \"\"\"\n",
    "#     Creates a sibling folder next to IMAGE_FOLDER that contains subset_size\n",
    "#     randomly selected images from IMAGE_FOLDER.\n",
    "    \n",
    "#     Example:\n",
    "#     IMAGE_FOLDER = \".../train2017\"\n",
    "#     Creates: \".../train2017_subset\"\n",
    "#     \"\"\"\n",
    "#     global IMAGE_FOLDER\n",
    "#     image_folder = Path(IMAGE_FOLDER)\n",
    "\n",
    "#     # Infer parent directory and base name\n",
    "#     parent_dir = image_folder.parent\n",
    "#     base_name = image_folder.name  # e.g., \"train2017\"\n",
    "\n",
    "#     # Default subset name = train2017_subset\n",
    "#     if subset_name is None:\n",
    "#         subset_name = f\"{base_name}_subset\"\n",
    "\n",
    "#     subset_path = parent_dir / subset_name\n",
    "#     subset_path.mkdir(exist_ok=True)\n",
    "\n",
    "#     # Collect valid images\n",
    "#     all_images = [\n",
    "#         f for f in image_folder.iterdir()\n",
    "#         if f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "#     ]\n",
    "\n",
    "#     if subset_size > len(all_images):\n",
    "#         raise ValueError(\n",
    "#             f\"Requested {subset_size} images but only {len(all_images)} available.\"\n",
    "#         )\n",
    "\n",
    "#     # Randomly choose subset\n",
    "#     selected = random.sample(all_images, subset_size)\n",
    "\n",
    "#     # Copy selected images into sibling folder\n",
    "#     for img in selected:\n",
    "#         shutil.copy(img, subset_path / img.name)\n",
    "\n",
    "#     print(f\"Created subset of {len(selected)} images at: {subset_path}\")\n",
    "    \n",
    "#     return subset_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qwen_outputs_exp1_multi(subset_size=None, last_turn_only=False):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(QWEN_EXP1_MULTI_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions_exp1(b64)\n",
    "                \n",
    "                # ---- CLEAN CAPTIONS HERE ----\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                history_correct = []\n",
    "\n",
    "                prev_attn = None\n",
    "        \n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, correct_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "                                \n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    #entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "                answers_incorrect = {}\n",
    "                mdi_incorrect = {}\n",
    "                entropy_incorrect = {}\n",
    "                shift_incorrect = {}\n",
    "                history_incorrect = []\n",
    "\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, incorrect_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "\n",
    "                    answers_incorrect[lvl] = ans\n",
    "                    mdi_incorrect[lvl] = round(mdi, 3)\n",
    "        \n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_incorrect[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    if prev_attn is None:\n",
    "                        shift_incorrect[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_incorrect[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    }, \n",
    "                    \n",
    "                    \"mdi_scores\": { \n",
    "                        \"correct_caption\": mdi_correct,     \n",
    "                        \"incorrect_caption\": mdi_incorrect \n",
    "                    },\n",
    "    \n",
    "                    \"entropy_scores\": {\n",
    "                        \"correct_caption\": entropy_correct,\n",
    "                        \"incorrect_caption\": entropy_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"shift_scores\": {\n",
    "                        \"correct_caption\": shift_correct,\n",
    "                        \"incorrect_caption\": shift_incorrect\n",
    "                    },\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {QWEN_EXP1_MULTI_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998be56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qwen_outputs_exp2_single(subset_size=None):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(QWEN_EXP2_SINGLE_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions_exp2(b64)\n",
    "                \n",
    "                # ---- CLEAN CAPTIONS HERE ----\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "                L4 = q[\"L4\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "\n",
    "                prev_attn = None\n",
    "        \n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3), (\"L4\", L4)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, correct_caption, q, return_metrics=True)\n",
    "                            \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    #entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"caption\": correct_caption,\n",
    "             \n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "                        \"L4\": L4\n",
    "                    },\n",
    "\n",
    "                    \"answers\": answers_correct,\n",
    "                    \n",
    "                    \"metrics\": {},\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "                levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "                \n",
    "                for lvl in levels:\n",
    "                    output[\"metrics\"][lvl] = {\n",
    "                    \"mdi\": mdi_correct.get(lvl),\n",
    "                    \"entropy\": entropy_correct.get(lvl),\n",
    "                    \"shift\": shift_correct.get(lvl)}\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"caption\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for lvl in levels:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    output[\"eval_scores\"][lvl] = score_c\n",
    "                    \n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {QWEN_EXP2_SINGLE_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89cfb5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_qwen_outputs_exp2_multi(subset_size=None, last_turn_only=False):\n",
    "    \"\"\"\n",
    "    If image_folder is provided (subset folder), use ALL images in that folder.\n",
    "    Otherwise, fall back to global IMAGE_FOLDER and apply subset_size logic.\n",
    "    \"\"\"\n",
    "#     global IMAGE_FOLDER\n",
    "\n",
    "#     # Determine which folder to read from\n",
    "#     active_folder = image_folder if image_folder is not None else IMAGE_FOLDER\n",
    "\n",
    "#     # List images in the selected folder\n",
    "#     all_image_files = [\n",
    "#         f for f in os.listdir(active_folder)\n",
    "#         if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "#     ]\n",
    "\n",
    "#     # CASE 1 — user passed a subset folder → ignore subset_size\n",
    "#     if image_folder is not None:\n",
    "#         image_files = all_image_files\n",
    "#         print(f\"Using all {len(image_files)} images from subset folder: {active_folder}\\n\")\n",
    "\n",
    "#     # CASE 2 — no subset folder → use normal random subset logic\n",
    "#     else:\n",
    "#         if subset_size is not None:\n",
    "#             image_files = random.sample(all_image_files, subset_size)\n",
    "#         else:\n",
    "#             image_files = all_image_files\n",
    "\n",
    "#         print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(QWEN_EXP2_MULTI_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions_exp2(b64)\n",
    "                \n",
    "                # ---- CLEAN CAPTIONS HERE ----\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "                L4 = q[\"L4\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                history_correct = []\n",
    "\n",
    "                prev_attn = None\n",
    "        \n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3), (\"L4\", L4)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, correct_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "                                \n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    #entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"caption\": correct_caption,\n",
    "             \n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "                        \"L4\": L4\n",
    "                    },\n",
    "\n",
    "                    \"answers\": answers_correct,\n",
    "                    \n",
    "                    \"metrics\": {},\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "                levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "                \n",
    "                for lvl in levels:\n",
    "                    output[\"metrics\"][lvl] = {\n",
    "                    \"mdi\": mdi_correct.get(lvl),\n",
    "                    \"entropy\": entropy_correct.get(lvl),\n",
    "                    \"shift\": shift_correct.get(lvl)}\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"caption\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for lvl in levels:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][lvl] = score_c\n",
    "                    \n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "    print(f\"\\nDone. JSONL saved to: {QWEN_EXP2_MULTI_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c113816",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ############## QWEN ##############\n",
    "    \n",
    "    subset_size = 1\n",
    "    generate_qwen_outputs_exp1_single(subset_size=subset_size)\n",
    "    generate_qwen_outputs_exp1_multi(subset_size=subset_size, last_turn_only=False)\n",
    "    \n",
    "    generate_qwen_outputs_exp2_single(subset_size=subset_size)\n",
    "    generate_qwen_outputs_exp2_multi(subset_size=subset_size, last_turn_only=False)\n",
    "    \n",
    "#     # Compute metrics for BLIP responses - make sure to change path accordingly!!\n",
    "#     qwen_pair_stats = pair_stats_by_level(QWEN_OUTPUT_PATH)\n",
    "#     qwen_fooling_rate_per_level = fooling_rate_by_level(qwen_pair_stats)\n",
    "#     qwen_acc_per_level = acc_by_level(qwen_pair_stats)\n",
    "    \n",
    "#     print(\"\\n========================\")\n",
    "#     print(\"FOOLING RATE PER LEVEL\")\n",
    "#     print(\"========================\\n\")\n",
    "#     for lvl, stats in qwen_fooling_rate_per_level.items():\n",
    "#         print(f\"{lvl}: Fooling Rate = {stats['fooled']}/{stats['total']} \"\n",
    "#               f\"({stats['rate']:.2f})\")\n",
    "\n",
    "#     print(\"\\n========================\")\n",
    "#     print(\"ACCURACY PER LEVEL\")\n",
    "#     print(\"========================\\n\")\n",
    "#     for lvl, stats in qwen_acc_per_level.items():\n",
    "#         print(f\"{lvl}:  \"\n",
    "#             f\"Acc(correct caption) = {stats['accuracy_correct_caption']:.2f},  \"\n",
    "#             f\"Acc(incorrect caption) = {stats['accuracy_incorrect_caption']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87ab30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
