{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea719672",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffaa8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dda562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0015a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "LLAVA_OUTPUT_PATH = \"llava_multi_exp2_responses.jsonl\" # CHANGE LATER\n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74c82e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c749db1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "============================================================\n",
    "1. Correct Caption\n",
    "============================================================\n",
    "‚Ä¢ Accurately describe the visible scene.\n",
    "‚Ä¢ 9‚Äì15 words, objective, simple, and factual.\n",
    "‚Ä¢ Should mention main objects; avoid inference beyond evidence.\n",
    "\n",
    "============================================================\n",
    "2. Visual Necessity Question Ladder (VNL): Levels L0 ‚Üí L4\n",
    "============================================================\n",
    "\n",
    "GENERAL RULES:\n",
    "‚Ä¢ L1‚ÄìL4 MUST require looking at the image to answer.\n",
    "‚Ä¢ All questions MUST be answerable using only the given image.\n",
    "‚Ä¢ Do NOT include the answers.\n",
    "‚Ä¢ No question should exceed 14 words.\n",
    "‚Ä¢ Return concise, natural wording.\n",
    "\n",
    "------------------------------------------------------------\n",
    "L0 ‚Äì Baseline Question (Language-prior only)\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ A question humans can answer **without seeing the image**.\n",
    "‚Ä¢ May refer to the world generally (NOT the specific image).\n",
    "‚Ä¢ Purpose: control for language-only biases.\n",
    "‚Ä¢ 6‚Äì12 words.\n",
    "Examples:\n",
    "‚Äì ‚ÄúWhat season often has the coldest weather?‚Äù  \n",
    "‚Äì ‚ÄúWhich animal is larger, a dog or an elephant?‚Äù  \n",
    "‚Äì ‚ÄúWhat do people usually use to take photographs?‚Äù\n",
    "\n",
    "------------------------------------------------------------\n",
    "L1 ‚Äì Basic Visual Recognition\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Requires the image.\n",
    "‚Ä¢ Ask about a **primary object** or its basic property.\n",
    "‚Ä¢ No reasoning, no inference.\n",
    "Examples:\n",
    "‚Äì ‚ÄúWhat object is the person holding?‚Äù  \n",
    "‚Äì ‚ÄúWhat color is the animal?‚Äù  \n",
    "‚Äì ‚ÄúHow many people are visible?‚Äù\n",
    "\n",
    "------------------------------------------------------------\n",
    "L2 ‚Äì Intermediate Visual Detail\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Also requires the image.\n",
    "‚Ä¢ Ask about a **secondary property** of a main object.\n",
    "‚Ä¢ Slightly more specific than L1.\n",
    "Examples:\n",
    "‚Äì ‚ÄúWhat pattern is on the person‚Äôs shirt?‚Äù  \n",
    "‚Äì ‚ÄúWhat type of hat is the man wearing?‚Äù  \n",
    "‚Äì ‚ÄúWhat material is the table made of?‚Äù\n",
    "\n",
    "------------------------------------------------------------\n",
    "L3 ‚Äì Relational / Spatial Reasoning\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Requires image + spatial relations + relational understanding.\n",
    "Examples:\n",
    "‚Äì ‚ÄúWhere is the dog positioned relative to the child?‚Äù  \n",
    "‚Äì ‚ÄúWhat object is behind the bicycle?‚Äù  \n",
    "‚Äì ‚ÄúWhich person is closest to the camera?‚Äù\n",
    "\n",
    "------------------------------------------------------------\n",
    "L4 ‚Äì High-Level Visual Reasoning\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Hardest level; requires the entire scene.\n",
    "‚Ä¢ Ask about interactions, goals, implied roles, or multi-object context.\n",
    "‚Ä¢ Still must be answerable from the image alone (no external inference).\n",
    "Examples:\n",
    "‚Äì ‚ÄúWhat activity are the people engaged in?‚Äù  \n",
    "‚Äì ‚ÄúWhy is the man extending his arm?‚Äù  \n",
    "‚Äì ‚ÄúWhat is the group collectively doing?‚Äù\n",
    "\n",
    "============================================================\n",
    "Return EXACTLY this JSON structure:\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",\n",
    "  \"L2\": \"<string>\",\n",
    "  \"L3\": \"<string>\",\n",
    "  \"L4\": \"<string>\"\n",
    "}\n",
    "============================================================\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# MAY NEED TO FIX! Outputs NaN sometimes\n",
    "\n",
    "def compute_attention_entropy(attentions):\n",
    "    \"\"\"\n",
    "    Computes normalized entropy of the final_attention vector.\n",
    "    Works with flattened attention tensors from LLaVA-1.5.\n",
    "    Returns a single float or None.\n",
    "    \"\"\"\n",
    "\n",
    "    if attentions is None:\n",
    "        return None\n",
    "\n",
    "    # flatten tuple-of-tuples into list of tensors\n",
    "    flat_attns = []\n",
    "    for layer in attentions:\n",
    "        if isinstance(layer, torch.Tensor):\n",
    "            flat_attns.append(layer)\n",
    "        elif isinstance(layer, (tuple, list)):\n",
    "            for x in layer:\n",
    "                if isinstance(x, torch.Tensor):\n",
    "                    flat_attns.append(x)\n",
    "\n",
    "    if not flat_attns:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in flat_attns:\n",
    "        if not isinstance(layer_attn, torch.Tensor) or layer_attn.ndim != 4:\n",
    "            continue\n",
    "\n",
    "        # avg over batch + heads ‚Üí [tgt_len, tgt_len]\n",
    "        attn = layer_attn.mean(dim=(0, 1))\n",
    "        final_attn = attn[-1]  # final token's attention distribution\n",
    "\n",
    "        if final_attn.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        p = final_attn / (final_attn.sum() + 1e-9)\n",
    "        p = p.clamp(min=1e-9)\n",
    "\n",
    "        entropy = -(p * p.log()).sum().item()\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    if not entropies:\n",
    "        return None\n",
    "\n",
    "    # average across layers\n",
    "    return sum(entropies) / len(entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_rows(attentions):\n",
    "    \"\"\"Helper: extract final-attention rows from flattened attention tensors.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    flat = []\n",
    "    for a in attentions:\n",
    "        if isinstance(a, torch.Tensor):\n",
    "            flat.append(a)\n",
    "        elif isinstance(a, (tuple, list)):\n",
    "            for b in a:\n",
    "                if isinstance(b, torch.Tensor):\n",
    "                    flat.append(b)\n",
    "\n",
    "    for layer_attn in flat:\n",
    "        if isinstance(layer_attn, torch.Tensor) and layer_attn.ndim == 4:\n",
    "            attn = layer_attn.mean(dim=(0,1))   # [tgt_len, tgt_len]\n",
    "            final_row = attn[-1]               # [tgt_len]\n",
    "            rows.append(final_row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# def compute_attention_shift(prev_attn, curr_attn):\n",
    "#     \"\"\"\n",
    "#     Computes the change in attention distribution between two turns.\n",
    "#     prev_attn and curr_attn are the raw attention objects returned by model.generate().\n",
    "#     \"\"\"\n",
    "\n",
    "#     if prev_attn is None or curr_attn is None:\n",
    "#         return None\n",
    "\n",
    "#     prev_rows = extract_final_rows(prev_attn)\n",
    "#     curr_rows = extract_final_rows(curr_attn)\n",
    "\n",
    "#     if len(prev_rows) == 0 or len(curr_rows) == 0:\n",
    "#         return None\n",
    "\n",
    "#     shifts = []\n",
    "\n",
    "#     # align by min number of layers\n",
    "#     for p, c in zip(prev_rows, curr_rows):\n",
    "#         # normalize\n",
    "#         p = p / (p.sum() + 1e-9)\n",
    "#         c = c / (c.sum() + 1e-9)\n",
    "\n",
    "#         # L1 distance\n",
    "#         shift = (p - c).abs().sum().item()\n",
    "#         shifts.append(shift)\n",
    "\n",
    "#     return sum(shifts) / len(shifts)\n",
    "\n",
    "def compute_attention_shift(prev_attn, curr_attn):\n",
    "    if prev_attn is None or curr_attn is None:\n",
    "        return None\n",
    "\n",
    "    prev_rows = extract_final_rows(prev_attn)\n",
    "    curr_rows = extract_final_rows(curr_attn)\n",
    "\n",
    "    if len(prev_rows) == 0 or len(curr_rows) == 0:\n",
    "        return None\n",
    "\n",
    "    shifts = []\n",
    "\n",
    "    for p, c in zip(prev_rows, curr_rows):\n",
    "        # normalize\n",
    "        p = p / (p.sum() + 1e-9)\n",
    "        c = c / (c.sum() + 1e-9)\n",
    "\n",
    "        # align lengths\n",
    "        L = min(p.shape[0], c.shape[0])\n",
    "        p = p[:L]\n",
    "        c = c[:L]\n",
    "\n",
    "        shift = (p - c).abs().sum().item()\n",
    "        shifts.append(shift)\n",
    "\n",
    "    return sum(shifts) / len(shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAJ \n",
    "\n",
    "def compute_llava_mdi(attentions, inputs, image_token_id=32000, vision_token_count=576):\n",
    "    \"\"\"\n",
    "    Computes MDI by detecting image token position and accounting for LLaVA's \n",
    "    internal token expansion (1 token -> 576 embeddings).\n",
    "    \"\"\"\n",
    "    if not attentions or len(attentions) == 0:\n",
    "        return None\n",
    "    \n",
    "    # 1. Find where the <image> token is in the input_ids\n",
    "    input_ids = inputs.input_ids[0]  # [seq_len]\n",
    "    image_indices = torch.where(input_ids == image_token_id)[0]\n",
    "    \n",
    "    if len(image_indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Position of <image> token in input_ids\n",
    "    img_token_pos = image_indices[0].item()\n",
    "    \n",
    "    # 2. Calculate actual position in attention matrix\n",
    "    # In input_ids: [token_0, token_1, ..., token_img_token_pos (=<image>), ..., token_n]\n",
    "    # In attention:  [token_0, token_1, ..., [576 vision embeddings], ..., token_n]    \n",
    "    vis_start = img_token_pos\n",
    "    vis_end = img_token_pos + vision_token_count\n",
    "\n",
    "    visual_scores = []\n",
    "    textual_scores = []\n",
    "    \n",
    "    # Iterate over generated tokens (outer tuple) and layers (inner tuple)\n",
    "    for token_step_attentions in attentions:\n",
    "        for layer_attention in token_step_attentions:\n",
    "            \n",
    "            # layer_attention shape: [batch, heads, query_len, key_len]\n",
    "            avg_attention = layer_attention.mean(dim=(0, 1)) \n",
    "            \n",
    "            # Attention of the newly generated token looking back at context\n",
    "            final_attn_row = avg_attention[-1, :]\n",
    "            \n",
    "            total_len = final_attn_row.shape[0]\n",
    "            \n",
    "            # Debug\n",
    "            # print(f\"Image starts at {vis_start}, ends at {vis_end}, Total Seq Len: {total_len}\")\n",
    "            \n",
    "            if vis_end <= total_len:\n",
    "                # Attention on visual tokens\n",
    "                visual_val = final_attn_row[vis_start:vis_end].sum().item()\n",
    "                \n",
    "                # Attention on text tokens (before and after image)\n",
    "                text_before = final_attn_row[:vis_start].sum().item()\n",
    "                text_after = final_attn_row[vis_end:].sum().item()\n",
    "                \n",
    "                textual_val = text_before + text_after\n",
    "            else:\n",
    "                # Fallback if dimensions don't match\n",
    "                visual_val = 0.0\n",
    "                textual_val = final_attn_row.sum().item()\n",
    "\n",
    "#             print(f\"Total attention length: {total_len}, \"\n",
    "#                   f\"Image tokens: [{vis_start}:{vis_end}], \"\n",
    "#                   f\"Visual attention: {visual_val:.4f}, \"\n",
    "#                   f\"Text attention: {textual_val:.4f}\")\n",
    "            visual_scores.append(visual_val)\n",
    "            textual_scores.append(textual_val)\n",
    "    \n",
    "    if not visual_scores:\n",
    "        return None\n",
    "    \n",
    "    # Average over all layers and generated tokens\n",
    "    avg_vis = sum(visual_scores) / len(visual_scores)\n",
    "    avg_text = sum(textual_scores) / len(textual_scores)\n",
    "    \n",
    "    # Compute MDI\n",
    "    mdi = avg_vis / (avg_vis + avg_text + 1e-9)\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c40857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEYA - V1\n",
    "\n",
    "def compute_llava_mdi(attentions, inputs, image_token_id=32000):\n",
    "    \"\"\"\n",
    "    MDI for a single-image LLaVA call.\n",
    "    MDI = (attention_on_vision_tokens) / (attention_on_all_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    if not attentions:\n",
    "        return None\n",
    "\n",
    "    # 1. Locate <image> token block (start/end)\n",
    "    img_positions = torch.where(inputs.input_ids[0] == image_token_id)[0]\n",
    "    if len(img_positions) == 0:\n",
    "        return None\n",
    "\n",
    "    img_start = img_positions[0].item()\n",
    "    img_end   = img_positions[-1].item() + 1   # non-inclusive\n",
    "\n",
    "    vision_scores = []\n",
    "    text_scores = []\n",
    "\n",
    "    # 2. Iterate over layers and generated tokens\n",
    "    for layer_attn in attentions:\n",
    "        for attn in layer_attn:\n",
    "            # attn shape = (batch, heads, 1, key_len)\n",
    "            attn = attn[0]                # ‚Üí (heads, 1, key_len)\n",
    "            attn = attn.mean(0)[0]        # ‚Üí (key_len,)\n",
    "\n",
    "            vis = attn[img_start:img_end].sum().item()\n",
    "            text = (attn[:img_start].sum() +\n",
    "                    attn[img_end:].sum()).item()\n",
    "\n",
    "            vision_scores.append(vis)\n",
    "            text_scores.append(text)\n",
    "\n",
    "    vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "    text_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "    mdi = vis_avg / (vis_avg + text_avg + 1e-9)\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b76ed2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ask_llava(\n",
    "    image_path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_metrics=True,\n",
    "    last_turn_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs LLaVA 1.5 with image + (caption + question) text prompt.\n",
    "    Supports:\n",
    "        - returning answer only\n",
    "        - returning answer + MDI\n",
    "        - returning answer + MDI + attention tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # ---- 1. Load Image ----\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---- 2. Build LLaVa-format prompt ----\n",
    "    # Structure similar to chat format\n",
    "#     text_prompt = (\n",
    "#         \"USER: <image>\\n\"\n",
    "#         f\"Context: {caption}\\n\"\n",
    "#         f\"Question: {question}\\n\"\n",
    "#         \"ASSISTANT:\"\n",
    "#     )\n",
    "\n",
    "    # START MULTI-TURN IMPLEMENTATION\n",
    "    prompt_parts = []\n",
    "    \n",
    "    if len(history) > 0:\n",
    "        for q_prev, a_prev in history:\n",
    "                prompt_parts.append(f\"USER: {q_prev}\\n\")\n",
    "                prompt_parts.append(f\"ASSISTANT: {a_prev}\\n\")\n",
    "    \n",
    "    prompt_parts.append(\"USER: <image>\\n\")\n",
    "    prompt_parts.append(f\"Context: {caption}\\n\")\n",
    "    prompt_parts.append(f\"Question: {question}\\n\")\n",
    "    prompt_parts.append(\"ASSISTANT:\")\n",
    "    \n",
    "    text_prompt = \"\".join(prompt_parts)\n",
    "    # END MULTI-TURN IMPLEMENTATION\n",
    "        \n",
    "\n",
    "    # ---- 3. Preprocess ---\n",
    "    inputs = processor(\n",
    "        text=text_prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "\n",
    "    # ---- 4. Generate with attention ----\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,          # deterministic\n",
    "            temperature=0.0,\n",
    "            output_attentions=True,   # ENABLE attentions\n",
    "            return_dict_in_generate=True,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "    # ---- 5. Decode the answer ----\n",
    "#     answer = processor.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_ids = outputs.sequences[0][inputs.input_ids.shape[1]:] # Slice off input\n",
    "    answer = processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Clean prefix\n",
    "    if \"ASSISTANT:\" in answer:\n",
    "        answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "    # 7. Compute Metrics\n",
    "    if return_metrics:\n",
    "        # Compute MDI - pass the entire attentions tuple and inputs\n",
    "        final_mdi = compute_llava_mdi(outputs.attentions, inputs) \n",
    "        if final_mdi is None:\n",
    "            final_mdi = 0.0\n",
    "\n",
    "        # Return tuple\n",
    "        return answer, final_mdi, outputs.attentions # ADDED output.attentions here\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"‚úÖ Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "print(\"Loading LLaVA model...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",   # üëà FORCE EAGER ATTENTION\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ LLaVA Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91389424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.language_model.model.layers[0].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26455cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "‚Ä¢ The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "‚Ä¢ Ignore any misleading or incorrect information in the CAPTION.\n",
    "‚Ä¢ Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "‚Ä¢ If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "‚Ä¢ If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "‚Ä¢ Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fee39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_llava_outputs(subset_size=None, last_turn_only=False):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(LLAVA_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "#                 incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "                L4 = q[\"L4\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                history_correct = []\n",
    "\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3), (\"L4\", L4)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn = ask_llava(path, correct_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "\n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    # entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"caption\": correct_caption,\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "                        \"L4\": L4\n",
    "                    },\n",
    "\n",
    "                    \"answers\": answers_correct,\n",
    "                    \n",
    "                    \"metrics\": {},\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "                levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "                \n",
    "                for lvl in levels:\n",
    "                    output[\"metrics\"][lvl] = {\n",
    "                    \"mdi\": mdi_correct.get(lvl),\n",
    "                    \"entropy\": entropy_correct.get(lvl),\n",
    "                    \"shift\": shift_correct.get(lvl)}\n",
    "                    \n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"caption\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = score_c\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {LLAVA_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ######## LLAVA ########\n",
    "    \n",
    "    generate_llava_outputs(subset_size=3, last_turn_only=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7526939",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
