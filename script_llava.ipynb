{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c80d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice1/1/8/ahanasoge6/CAI/Project\n",
      "/storage/ice1/1/8/ahanasoge6/CAI/Project\r\n"
     ]
    }
   ],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/ahanasoge6/scratch/CAI/Project\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c62e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai\n",
    "!pip install -q transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4d542e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/ahanasoge6/.local/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.13) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d683c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "GOOGLE_API_KEY = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaacb34",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc8f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models for your key:\n",
      "models/gemini-2.5-pro-vtea-da-csi\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "Available models for your key:\n",
      "models/gemini-2.5-pro-vtea-da-csi\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n"
     ]
    }
   ],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Google API Key is missing\")\n",
    "    \n",
    "print(\"Available models for your key:\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)\n",
    "    \n",
    "print(\"Available models for your key:\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd5a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"images\"           \n",
    "#OUTPUT_PATH = \"blip_results/captions.jsonl\"\n",
    "OUTPUT_PATH = \"llava_results/captions.jsonl\"\n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "GOOGLE_MODEL = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e88a84c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965959b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "• Must accurately describe the visible scene.\n",
    "• 7–15 words, objective, simple, and factual.\n",
    "• Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "• Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "• Must keep the same length and sentence structure style as the correct caption.\n",
    "• MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     – species/category of the main object\n",
    "     – color of a main object\n",
    "     – pattern/texture of a main object\n",
    "     – object type that a person is holding/using\n",
    "     – action the main subject is performing\n",
    "     – spatial relation (e.g., “in front of” → “behind”)\n",
    "     \n",
    "• The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, “ocean” is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "• The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "• The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0–L4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 — Pure language prior  \n",
    "• Must be answerable with NO access to the image.  \n",
    "• General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "• 6–14 words.\n",
    "\n",
    "L1 — Probe changed attribute #1 \n",
    "• MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "• Example:If species changed, ask “What type of animal…?”  \n",
    "          If color changed, ask “What color is…?”  \n",
    "          If object type changed, ask “What object is… holding?”  \n",
    "• No attributes other than the first changed one.  \n",
    "• 6–14 words.\n",
    "\n",
    "L2 — Probe changed attribute #2\n",
    "• MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "• Same rules as L2 but targeting the second changed detail.  \n",
    "• Should not be the same question as L1. \n",
    "• 6–14 words.\n",
    "\n",
    "L3 — High-level reasoning\n",
    "• Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "• The question MUST NOT depend on the two changed attributes.\n",
    "• The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "• The question SHOULD require general common-sense or contextual reasoning.\n",
    "• The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "• 6–14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "• Do NOT provide answers.\n",
    "• Do NOT describe the image outside captions.\n",
    "• All questions must be 6–14 words.\n",
    "• Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf6c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask_blip2(image_path, caption, question, max_new_tokens=50):\n",
    "#     \"\"\"\n",
    "#     Runs BLIP-2 Flan-T5-xl on:\n",
    "#         IMAGE + (caption + question) text prompt.\n",
    "#     Returns: the generated answer as a clean string.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---- Load image ----\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#     # ---- Build prompt for BLIP-2 ----\n",
    "#     # Format:  \"<caption>\\n\\nQuestion: <question>\\nAnswer:\"\n",
    "#     prompt = f\"{caption}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "#     # ---- Preprocess ----\n",
    "#     inputs = processor(\n",
    "#         image,\n",
    "#         prompt,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "#     # ---- Generate answer ----\n",
    "#     output_ids = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         temperature=0.0,      # deterministic & stable (recommended)\n",
    "#         do_sample=False\n",
    "#     )\n",
    "\n",
    "#     # ---- Decode ----\n",
    "#     answer = processor.tokenizer.decode(\n",
    "#         output_ids[0],\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "#     # ---- Strip the prompt part (BLIP-2 often echoes input) ----\n",
    "#     if \"Answer:\" in answer:\n",
    "#         answer = answer.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "#     # Clean spacing\n",
    "#     return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14a5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llava(image_path, caption, question, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Runs LLaVA-1.5 on:\n",
    "        IMAGE + (caption + question) text prompt.\n",
    "    \"\"\"\n",
    "    # 1. Load Image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # 2. Format Prompt (LLaVA specific chat format)\n",
    "    # Structure: USER: <image>\\nContext: {caption}\\nQuestion: {question}\\nAnswer the question concisely.\\nASSISTANT:\n",
    "    text_prompt = f\"USER: <image>\\nContext: {caption}\\nQuestion: {question}\\nAnswer the question concisely.\\nASSISTANT:\"\n",
    "\n",
    "    # 3. Preprocess\n",
    "    inputs = processor(\n",
    "        text=text_prompt, \n",
    "        images=image, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    # 4. Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, # Deterministic for testing\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    # 5. Decode\n",
    "    answer = processor.decode(\n",
    "        output_ids[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # 6. Clean up output (Remove the input prompt from the result)\n",
    "    if \"ASSISTANT:\" in answer:\n",
    "        answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e351b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/ahanasoge6/scratch/CAI/Project/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/ahanasoge6/scratch/CAI/Project/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/ahanasoge6/scratch/CAI/Project/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2d68a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/ahanasoge6/.local/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f5f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"✅ Using device:\", device)\n",
    "\n",
    "# # \"Salesforce/blip2-flan-t5-xxl\" -> bigger, might need 2x memory\n",
    "# MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.eval()\n",
    "# print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8003aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n",
      "Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLaVA Model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "print(\"Loading LLaVA model...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device)\n",
    "\n",
    "print(\"✅ LLaVA Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b19a406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f0da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "#     \"\"\"\n",
    "#     Builds the judge prompt, encodes the image,\n",
    "#     calls Claude directly, and returns 0 or 1.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---- Build prompt ----\n",
    "#     prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "#     # ---- Encode image ----\n",
    "#     with open(image_path, \"rb\") as f:\n",
    "#         img_bytes = f.read()\n",
    "#     b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "#     # ---- Call Claude ----\n",
    "#     response = anthropic_client.messages.create(\n",
    "#         model=\"claude-sonnet-4-5-20250929\",\n",
    "#         max_tokens=5,\n",
    "#         temperature=0,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": \"image/jpeg\",\n",
    "#                             \"data\": b64img\n",
    "#                         }\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # ---- Parse output ----\n",
    "#     output = response.content[0].text.strip()\n",
    "\n",
    "#     if output not in (\"0\", \"1\"):\n",
    "#         raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "#     return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c5b27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Uses Google Gemini to judge the answer.\n",
    "    Returns 0 or 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load the image using PIL (Gemini likes this format)\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # 2. Get your prompt text\n",
    "    # (Keep your create_eval_prompt function exactly the same as it is now)\n",
    "    prompt_text = create_eval_prompt(caption, condition, question, model_answer)\n",
    "    \n",
    "    try:\n",
    "        # 3. Call Gemini\n",
    "        # We pass the text prompt AND the image object in a list\n",
    "        response = GOOGLE_MODEL.generate_content([prompt_text, img])\n",
    "        \n",
    "        # 4. Clean the output\n",
    "        output = response.text.strip()\n",
    "        \n",
    "        # Gemini might be chatty, ensure we just get the number\n",
    "        if \"1\" in output: return 1\n",
    "        if \"0\" in output: return 0\n",
    "        \n",
    "        # Fallback if it returns something else\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Gemini Error: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7b572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOWER - doesn't parallelize 8 API calls \n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "    \n",
    "    # --- NEW: Create the output directory if it doesn't exist ---\n",
    "    output_dir = os.path.dirname(OUTPUT_PATH)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"✅ Created output folder: {output_dir}\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption   = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "#                 L0, L1, L2, L3, L4 = q[\"L0\"], q[\"L1\"], q[\"L2\"], q[\"L3\"], q[\"L4\"]\n",
    "                L0, L1, L2, L3 = q[\"L0\"], q[\"L1\"], q[\"L2\"], q[\"L3\"]\n",
    "\n",
    "\n",
    "#                 # ---- 2) BLIP-2 answers ----\n",
    "#                 answers_correct = {\n",
    "#                     \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, correct_caption, L3),\n",
    "# #                     \"L4\": ask_blip2(path, correct_caption, L4),\n",
    "#                 }\n",
    "\n",
    "#                 answers_incorrect = {\n",
    "#                     \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, incorrect_caption, L3),\n",
    "# #                     \"L4\": ask_blip2(path, incorrect_caption, L4),\n",
    "#                 }\n",
    "\n",
    "                # ---- 2) LLaVA answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_llava(path, correct_caption, L0),\n",
    "                    \"L1\": ask_llava(path, correct_caption, L1),\n",
    "                    \"L2\": ask_llava(path, correct_caption, L2),\n",
    "                    \"L3\": ask_llava(path, correct_caption, L3),\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_llava(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_llava(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_llava(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_llava(path, incorrect_caption, L3),\n",
    "                }\n",
    "\n",
    "                # ---- 3) Write compact JSON ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "#                         \"L4\": L4\n",
    "                    },\n",
    "                    \"answers\": {\n",
    "                        \"correct\": answers_correct,\n",
    "                        \"incorrect\": answers_incorrect\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "#                 Evaluate responses with Claude-3.5 Sonnet\n",
    "                for level, question in output[\"questions\"].items():\n",
    "\n",
    "                    # correct caption condition\n",
    "                    score_c = eval_answer(\n",
    "                        path,\n",
    "                        output[\"captions\"][\"correct\"],\n",
    "                        \"correct caption condition\",\n",
    "                        question,\n",
    "                        output[\"answers\"][\"correct\"][level]\n",
    "                    )\n",
    "\n",
    "                    # incorrect caption condition\n",
    "                    score_i = eval_answer(\n",
    "                        path,\n",
    "                        output[\"captions\"][\"incorrect\"],\n",
    "                        \"incorrect caption condition\",\n",
    "                        question,\n",
    "                        output[\"answers\"][\"incorrect\"][level]\n",
    "                    )\n",
    "\n",
    "                    all_results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"level\": level,\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    })\n",
    "                    \n",
    "                print(all_results)\n",
    "            \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n Error with {image_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99ee5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTER - Parallelizes API calls\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption   = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "#                 answers_correct = {\n",
    "#                     \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, correct_caption, L3),\n",
    "#                 }\n",
    "\n",
    "#                 answers_incorrect = {\n",
    "#                     \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, incorrect_caption, L3),\n",
    "#                 }\n",
    "\n",
    "                # ---- 2) LLaVA answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_llava(path, correct_caption, L0),\n",
    "                    \"L1\": ask_llava(path, correct_caption, L1),\n",
    "                    \"L2\": ask_llava(path, correct_caption, L2),\n",
    "                    \"L3\": ask_llava(path, correct_caption, L3),\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_llava(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_llava(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_llava(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_llava(path, incorrect_caption, L3),\n",
    "                }\n",
    "\n",
    "                # ---- 3) Write compact JSON ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "                    \"answers\": {\n",
    "                        \"correct\": answers_correct,\n",
    "                        \"incorrect\": answers_incorrect\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                #with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                #Using 2 workers for gemini free tier\n",
    "                with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect\"][level]\n",
    "                        ))\n",
    "\n",
    "                    results = [j.result() for j in as_completed(jobs)]\n",
    "\n",
    "                # results arrive unordered → rebuild matching pairs\n",
    "                # We submitted jobs in pairs, so their order is known\n",
    "                ordered_results = []\n",
    "                for j in jobs:\n",
    "                    ordered_results.append(j.result())\n",
    "\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    all_results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"level\": level,\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n Error with {image_id}: {e}\")\n",
    "\n",
    "    print(all_results)\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "957c72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fooling_by_level(jsonl_path):\n",
    "    stats = {\n",
    "        \"L0\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L1\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L2\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L3\": {\"fooled\": 0, \"total\": 0}\n",
    "    }\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            # ensure consistent level order\n",
    "            for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                scores = eval_scores[level]\n",
    "\n",
    "                s_c = scores[\"correct_caption_score\"]\n",
    "                s_i = scores[\"incorrect_caption_score\"]\n",
    "\n",
    "                stats[level][\"total\"] += 1\n",
    "\n",
    "                # Fooling event = correct under correct caption AND incorrect under incorrect caption\n",
    "                if s_c == 1 and s_i == 0:\n",
    "                    stats[level][\"fooled\"] += 1\n",
    "\n",
    "    # compute fooling rates\n",
    "    for lvl in stats:\n",
    "        total = stats[lvl][\"total\"]\n",
    "        fooled = stats[lvl][\"fooled\"]\n",
    "        stats[lvl][\"rate\"] = fooled / total if total > 0 else 0.0\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77756bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "#                 answers_correct = {\n",
    "#                     \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, correct_caption, L3)\n",
    "#                 }\n",
    "\n",
    "#                 answers_incorrect = {\n",
    "#                     \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "#                     \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "#                     \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "#                     \"L3\": ask_blip2(path, incorrect_caption, L3)\n",
    "#                 }\n",
    "\n",
    "                # ---- 2) LLaVA answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_llava(path, correct_caption, L0),\n",
    "                    \"L1\": ask_llava(path, correct_caption, L1),\n",
    "                    \"L2\": ask_llava(path, correct_caption, L2),\n",
    "                    \"L3\": ask_llava(path, correct_caption, L3),\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_llava(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_llava(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_llava(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_llava(path, incorrect_caption, L3),\n",
    "                }\n",
    "            \n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"eval_scores\": {}   # will be filled next\n",
    "                }\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\\n\")\n",
    "    stats = compute_fooling_by_level(OUTPUT_PATH)\n",
    "    print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5999a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Processing: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. JSONL saved to: llava_results/captions.jsonl\n",
      "\n",
      "{'L0': {'fooled': 0, 'total': 1, 'rate': 0.0}, 'L1': {'fooled': 1, 'total': 1, 'rate': 1.0}, 'L2': {'fooled': 0, 'total': 1, 'rate': 0.0}, 'L3': {'fooled': 1, 'total': 1, 'rate': 1.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c36822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
