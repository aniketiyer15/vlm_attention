{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8eaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "LLAVA_OUTPUT_PATH = \"llava_multi_exp1_responses.jsonl\" # CHANGE LATER\n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38c09c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96ad24",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "‚Ä¢ Must accurately describe the visible scene.\n",
    "‚Ä¢ 7‚Äì15 words, objective, simple, and factual.\n",
    "‚Ä¢ Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "‚Ä¢ Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "‚Ä¢ Must keep the same length and sentence structure style as the correct caption.\n",
    "‚Ä¢ MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     ‚Äì species/category of the main object\n",
    "     ‚Äì color of a main object\n",
    "     ‚Äì pattern/texture of a main object\n",
    "     ‚Äì object type that a person is holding/using\n",
    "     ‚Äì action the main subject is performing\n",
    "     ‚Äì spatial relation (e.g., ‚Äúin front of‚Äù ‚Üí ‚Äúbehind‚Äù)\n",
    "     \n",
    "‚Ä¢ The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, ‚Äúocean‚Äù is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "‚Ä¢ The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "‚Ä¢ The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0‚ÄìL4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 ‚Äî Pure language prior  \n",
    "‚Ä¢ Must be answerable with NO access to the image.  \n",
    "‚Ä¢ General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "‚Ä¢ 6‚Äì14 words.\n",
    "\n",
    "L1 ‚Äî Probe changed attribute #1 \n",
    "‚Ä¢ MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "‚Ä¢ Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "‚Ä¢ Example:If species changed, ask ‚ÄúWhat type of animal‚Ä¶?‚Äù  \n",
    "          If color changed, ask ‚ÄúWhat color is‚Ä¶?‚Äù  \n",
    "          If object type changed, ask ‚ÄúWhat object is‚Ä¶ holding?‚Äù  \n",
    "‚Ä¢ No attributes other than the first changed one.  \n",
    "‚Ä¢ 6‚Äì14 words.\n",
    "\n",
    "L2 ‚Äî Probe changed attribute #2\n",
    "‚Ä¢ MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "‚Ä¢ Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "‚Ä¢ Same rules as L2 but targeting the second changed detail.  \n",
    "‚Ä¢ Should not be the same question as L1. \n",
    "‚Ä¢ 6‚Äì14 words.\n",
    "\n",
    "L3 ‚Äî High-level reasoning\n",
    "‚Ä¢ Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "‚Ä¢ The question MUST NOT depend on the two changed attributes.\n",
    "‚Ä¢ The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "‚Ä¢ The question SHOULD require general common-sense or contextual reasoning.\n",
    "‚Ä¢ The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "‚Ä¢ 6‚Äì14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "‚Ä¢ Do NOT provide answers.\n",
    "‚Ä¢ Do NOT describe the image outside captions.\n",
    "‚Ä¢ All questions must be 6‚Äì14 words.\n",
    "‚Ä¢ Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1979ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def compute_attention_entropy(attentions):\n",
    "    \"\"\"\n",
    "    Computes normalized entropy of the final_attention vector.\n",
    "    Works with flattened attention tensors from LLaVA-1.5.\n",
    "    Returns a single float or None.\n",
    "    \"\"\"\n",
    "\n",
    "    if attentions is None:\n",
    "        return None\n",
    "\n",
    "    # flatten tuple-of-tuples into list of tensors\n",
    "    flat_attns = []\n",
    "    for layer in attentions:\n",
    "        if isinstance(layer, torch.Tensor):\n",
    "            flat_attns.append(layer)\n",
    "        elif isinstance(layer, (tuple, list)):\n",
    "            for x in layer:\n",
    "                if isinstance(x, torch.Tensor):\n",
    "                    flat_attns.append(x)\n",
    "\n",
    "    if not flat_attns:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in flat_attns:\n",
    "        if not isinstance(layer_attn, torch.Tensor) or layer_attn.ndim != 4:\n",
    "            continue\n",
    "\n",
    "        # avg over batch + heads ‚Üí [tgt_len, tgt_len]\n",
    "        attn = layer_attn.mean(dim=(0, 1))\n",
    "        final_attn = attn[-1]  # final token's attention distribution\n",
    "\n",
    "        if final_attn.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        p = final_attn / (final_attn.sum() + 1e-9)\n",
    "        p = p.clamp(min=1e-9)\n",
    "\n",
    "        entropy = -(p * p.log()).sum().item()\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    if not entropies:\n",
    "        return None\n",
    "\n",
    "    # average across layers\n",
    "    return sum(entropies) / len(entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef82080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_rows(attentions):\n",
    "    \"\"\"Helper: extract final-attention rows from flattened attention tensors.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    flat = []\n",
    "    for a in attentions:\n",
    "        if isinstance(a, torch.Tensor):\n",
    "            flat.append(a)\n",
    "        elif isinstance(a, (tuple, list)):\n",
    "            for b in a:\n",
    "                if isinstance(b, torch.Tensor):\n",
    "                    flat.append(b)\n",
    "\n",
    "    for layer_attn in flat:\n",
    "        if isinstance(layer_attn, torch.Tensor) and layer_attn.ndim == 4:\n",
    "            attn = layer_attn.mean(dim=(0,1))   # [tgt_len, tgt_len]\n",
    "            final_row = attn[-1]               # [tgt_len]\n",
    "            rows.append(final_row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# def compute_attention_shift(prev_attn, curr_attn):\n",
    "#     \"\"\"\n",
    "#     Computes the change in attention distribution between two turns.\n",
    "#     prev_attn and curr_attn are the raw attention objects returned by model.generate().\n",
    "#     \"\"\"\n",
    "\n",
    "#     if prev_attn is None or curr_attn is None:\n",
    "#         return None\n",
    "\n",
    "#     prev_rows = extract_final_rows(prev_attn)\n",
    "#     curr_rows = extract_final_rows(curr_attn)\n",
    "\n",
    "#     if len(prev_rows) == 0 or len(curr_rows) == 0:\n",
    "#         return None\n",
    "\n",
    "#     shifts = []\n",
    "\n",
    "#     # align by min number of layers\n",
    "#     for p, c in zip(prev_rows, curr_rows):\n",
    "#         # normalize\n",
    "#         p = p / (p.sum() + 1e-9)\n",
    "#         c = c / (c.sum() + 1e-9)\n",
    "\n",
    "#         # L1 distance\n",
    "#         shift = (p - c).abs().sum().item()\n",
    "#         shifts.append(shift)\n",
    "\n",
    "#     return sum(shifts) / len(shifts)\n",
    "\n",
    "def compute_attention_shift(prev_attn, curr_attn):\n",
    "    if prev_attn is None or curr_attn is None:\n",
    "        return None\n",
    "\n",
    "    prev_rows = extract_final_rows(prev_attn)\n",
    "    curr_rows = extract_final_rows(curr_attn)\n",
    "\n",
    "    if len(prev_rows) == 0 or len(curr_rows) == 0:\n",
    "        return None\n",
    "\n",
    "    shifts = []\n",
    "\n",
    "    for p, c in zip(prev_rows, curr_rows):\n",
    "        # normalize\n",
    "        p = p / (p.sum() + 1e-9)\n",
    "        c = c / (c.sum() + 1e-9)\n",
    "\n",
    "        # align lengths\n",
    "        L = min(p.shape[0], c.shape[0])\n",
    "        p = p[:L]\n",
    "        c = c[:L]\n",
    "\n",
    "        shift = (p - c).abs().sum().item()\n",
    "        shifts.append(shift)\n",
    "\n",
    "    return sum(shifts) / len(shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3237f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAJ \n",
    "\n",
    "def compute_llava_mdi(attentions, inputs, image_token_id=32000, vision_token_count=576):\n",
    "    \"\"\"\n",
    "    Computes MDI by detecting image token position and accounting for LLaVA's \n",
    "    internal token expansion (1 token -> 576 embeddings).\n",
    "    \"\"\"\n",
    "    if not attentions or len(attentions) == 0:\n",
    "        return None\n",
    "    \n",
    "    # 1. Find where the <image> token is in the input_ids\n",
    "    input_ids = inputs.input_ids[0]  # [seq_len]\n",
    "    image_indices = torch.where(input_ids == image_token_id)[0]\n",
    "    \n",
    "    if len(image_indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Position of <image> token in input_ids\n",
    "    img_token_pos = image_indices[0].item()\n",
    "    \n",
    "    # 2. Calculate actual position in attention matrix\n",
    "    # In input_ids: [token_0, token_1, ..., token_img_token_pos (=<image>), ..., token_n]\n",
    "    # In attention:  [token_0, token_1, ..., [576 vision embeddings], ..., token_n]    \n",
    "    vis_start = img_token_pos\n",
    "    vis_end = img_token_pos + vision_token_count\n",
    "\n",
    "    visual_scores = []\n",
    "    textual_scores = []\n",
    "    \n",
    "    # Iterate over generated tokens (outer tuple) and layers (inner tuple)\n",
    "    for token_step_attentions in attentions:\n",
    "        for layer_attention in token_step_attentions:\n",
    "            \n",
    "            # layer_attention shape: [batch, heads, query_len, key_len]\n",
    "            avg_attention = layer_attention.mean(dim=(0, 1)) \n",
    "            \n",
    "            # Attention of the newly generated token looking back at context\n",
    "            final_attn_row = avg_attention[-1, :]\n",
    "            \n",
    "            total_len = final_attn_row.shape[0]\n",
    "            \n",
    "            # Debug\n",
    "            # print(f\"Image starts at {vis_start}, ends at {vis_end}, Total Seq Len: {total_len}\")\n",
    "            \n",
    "            if vis_end <= total_len:\n",
    "                # Attention on visual tokens\n",
    "                visual_val = final_attn_row[vis_start:vis_end].sum().item()\n",
    "                \n",
    "                # Attention on text tokens (before and after image)\n",
    "                text_before = final_attn_row[:vis_start].sum().item()\n",
    "                text_after = final_attn_row[vis_end:].sum().item()\n",
    "                \n",
    "                textual_val = text_before + text_after\n",
    "            else:\n",
    "                # Fallback if dimensions don't match\n",
    "                visual_val = 0.0\n",
    "                textual_val = final_attn_row.sum().item()\n",
    "\n",
    "#             print(f\"Total attention length: {total_len}, \"\n",
    "#                   f\"Image tokens: [{vis_start}:{vis_end}], \"\n",
    "#                   f\"Visual attention: {visual_val:.4f}, \"\n",
    "#                   f\"Text attention: {textual_val:.4f}\")\n",
    "            visual_scores.append(visual_val)\n",
    "            textual_scores.append(textual_val)\n",
    "    \n",
    "    if not visual_scores:\n",
    "        return None\n",
    "    \n",
    "    # Average over all layers and generated tokens\n",
    "    avg_vis = sum(visual_scores) / len(visual_scores)\n",
    "    avg_text = sum(textual_scores) / len(textual_scores)\n",
    "    \n",
    "    # Compute MDI\n",
    "    mdi = avg_vis / (avg_vis + avg_text + 1e-9)\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a30005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEYA - V1\n",
    "\n",
    "def compute_llava_mdi(attentions, inputs, image_token_id=32000):\n",
    "    \"\"\"\n",
    "    MDI for a single-image LLaVA call.\n",
    "    MDI = (attention_on_vision_tokens) / (attention_on_all_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    if not attentions:\n",
    "        return None\n",
    "\n",
    "    # 1. Locate <image> token block (start/end)\n",
    "    img_positions = torch.where(inputs.input_ids[0] == image_token_id)[0]\n",
    "    if len(img_positions) == 0:\n",
    "        return None\n",
    "\n",
    "    img_start = img_positions[0].item()\n",
    "    img_end   = img_positions[-1].item() + 1   # non-inclusive\n",
    "\n",
    "    vision_scores = []\n",
    "    text_scores = []\n",
    "\n",
    "    # 2. Iterate over layers and generated tokens\n",
    "    for layer_attn in attentions:\n",
    "        for attn in layer_attn:\n",
    "            # attn shape = (batch, heads, 1, key_len)\n",
    "            attn = attn[0]                # ‚Üí (heads, 1, key_len)\n",
    "            attn = attn.mean(0)[0]        # ‚Üí (key_len,)\n",
    "\n",
    "            vis = attn[img_start:img_end].sum().item()\n",
    "            text = (attn[:img_start].sum() +\n",
    "                    attn[img_end:].sum()).item()\n",
    "\n",
    "            vision_scores.append(vis)\n",
    "            text_scores.append(text)\n",
    "\n",
    "    vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "    text_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "    mdi = vis_avg / (vis_avg + text_avg + 1e-9)\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef362624",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ask_llava(\n",
    "    image_path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_metrics=True,\n",
    "    last_turn_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs LLaVA 1.5 with image + (caption + question) text prompt.\n",
    "    Supports:\n",
    "        - returning answer only\n",
    "        - returning answer + MDI\n",
    "        - returning answer + MDI + attention tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # ---- 1. Load Image ----\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---- 2. Build LLaVa-format prompt ----\n",
    "    # Structure similar to chat format\n",
    "#     text_prompt = (\n",
    "#         \"USER: <image>\\n\"\n",
    "#         f\"Context: {caption}\\n\"\n",
    "#         f\"Question: {question}\\n\"\n",
    "#         \"ASSISTANT:\"\n",
    "#     )\n",
    "\n",
    "    # START MULTI-TURN IMPLEMENTATION\n",
    "    prompt_parts = []\n",
    "    \n",
    "    if len(history) > 0:\n",
    "        for q_prev, a_prev in history:\n",
    "                prompt_parts.append(f\"USER: {q_prev}\\n\")\n",
    "                prompt_parts.append(f\"ASSISTANT: {a_prev}\\n\")\n",
    "    \n",
    "    prompt_parts.append(\"USER: <image>\\n\")\n",
    "    prompt_parts.append(f\"Context: {caption}\\n\")\n",
    "    prompt_parts.append(f\"Question: {question}\\n\")\n",
    "    prompt_parts.append(\"ASSISTANT:\")\n",
    "    \n",
    "    text_prompt = \"\".join(prompt_parts)\n",
    "    # END MULTI-TURN IMPLEMENTATION\n",
    "        \n",
    "\n",
    "    # ---- 3. Preprocess ---\n",
    "    inputs = processor(\n",
    "        text=text_prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "\n",
    "    # ---- 4. Generate with attention ----\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,          # deterministic\n",
    "            temperature=0.0,\n",
    "            output_attentions=True,   # ENABLE attentions\n",
    "            return_dict_in_generate=True,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "    # ---- 5. Decode the answer ----\n",
    "#     answer = processor.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_ids = outputs.sequences[0][inputs.input_ids.shape[1]:] # Slice off input\n",
    "    answer = processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Clean prefix\n",
    "    if \"ASSISTANT:\" in answer:\n",
    "        answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "    # 7. Compute Metrics\n",
    "    if return_metrics:\n",
    "        # Compute MDI - pass the entire attentions tuple and inputs\n",
    "        final_mdi = compute_llava_mdi(outputs.attentions, inputs) \n",
    "        if final_mdi is None:\n",
    "            final_mdi = 0.0\n",
    "\n",
    "        # Return tuple\n",
    "        return answer, final_mdi, outputs.attentions # ADDED output.attentions here\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"‚úÖ Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "print(\"Loading LLaVA model...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",   # üëà FORCE EAGER ATTENTION\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ LLaVA Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.language_model.model.layers[0].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "‚Ä¢ The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "‚Ä¢ Ignore any misleading or incorrect information in the CAPTION.\n",
    "‚Ä¢ Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "‚Ä¢ If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "‚Ä¢ If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "‚Ä¢ Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fbc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to setup eval metric calculation\n",
    "\n",
    "def pair_stats_by_level(jsonl_path):\n",
    "    levels = [\"L0\", \"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "    # Tallies per level\n",
    "    pair_stats = {\n",
    "        lvl: {(1,1):0, (1,0):0, (0,1):0, (0,0):0}\n",
    "        for lvl in levels\n",
    "    }\n",
    "\n",
    "    # ---- Single JSONL pass ----\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            for lvl in levels:\n",
    "                s_c = eval_scores[lvl][\"correct_caption_score\"]\n",
    "                s_i = eval_scores[lvl][\"incorrect_caption_score\"]\n",
    "                pair_stats[lvl][(s_c, s_i)] += 1\n",
    "\n",
    "    return pair_stats\n",
    "\n",
    "def conf_pairs_by_level(pair_stats):\n",
    "    return pair_stats  # already exactly the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metric - fooling rate by level \n",
    "# When the model answers correctly in the correct-caption condition but answers incorrectly in the incorrect-caption condition.\n",
    "\n",
    "def fooling_rate_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c10 = counts[(1,0)]\n",
    "        total = sum(counts.values())\n",
    "        rate = c10 / total if total > 0 else 0\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"fooled\": c10,\n",
    "            \"total\": total,\n",
    "            \"rate\": rate,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d429d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metrics - per-level answer accuracy, computed separately for the correct-caption and incorrect-caption conditions.\n",
    "\n",
    "def acc_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c11 = counts[(1,1)]\n",
    "        c10 = counts[(1,0)]\n",
    "        c01 = counts[(0,1)]\n",
    "        c00 = counts[(0,0)]\n",
    "        total = c11 + c10 + c01 + c00\n",
    "\n",
    "        if total == 0:\n",
    "            results[lvl] = None\n",
    "            continue\n",
    "\n",
    "        # accuracy under correct caption = model is correct (regardless of incorrect-caption score)\n",
    "        acc_correct = (c11 + c10) / total\n",
    "        # accuracy under incorrect caption = model is correct under wrong caption\n",
    "        acc_incorrect = (c11 + c01) / total\n",
    "\n",
    "#         mdi = acc_correct - acc_incorrect\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"accuracy_correct_caption\": acc_correct,\n",
    "            \"accuracy_incorrect_caption\": acc_incorrect\n",
    "#             \"MDI\": mdi,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_llava_outputs(subset_size=None, last_turn_only=False):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(LLAVA_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                history_correct = []\n",
    "\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn = ask_llava(path, correct_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "\n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    # entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "                answers_incorrect = {}\n",
    "                mdi_incorrect = {}\n",
    "                entropy_incorrect = {}\n",
    "                shift_incorrect = {}\n",
    "                history_incorrect = []\n",
    "\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "#                     ans, mdi, attn = ask_blip2(path, incorrect_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn = ask_llava(path, incorrect_caption, q, return_metrics=True, last_turn_only=last_turn_only)\n",
    "\n",
    "                    answers_incorrect[lvl] = ans\n",
    "                    mdi_incorrect[lvl] = round(mdi, 3)\n",
    "        \n",
    "                    if last_turn_only: \n",
    "                        history_correct = [(q, ans)]\n",
    "                    else:\n",
    "                        history_correct.append((q, ans))\n",
    "                        \n",
    "\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_incorrect[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    if prev_attn is None:\n",
    "                        shift_incorrect[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_incorrect[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    }, \n",
    "                    \n",
    "                    \"mdi_scores\": { \n",
    "                        \"correct_caption\": mdi_correct,     \n",
    "                        \"incorrect_caption\": mdi_incorrect \n",
    "                    },\n",
    "    \n",
    "                    \"entropy_scores\": {\n",
    "                        \"correct_caption\": entropy_correct,\n",
    "                        \"incorrect_caption\": entropy_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"shift_scores\": {\n",
    "                        \"correct_caption\": shift_correct,\n",
    "                        \"incorrect_caption\": shift_incorrect\n",
    "                    },\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {LLAVA_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ce109",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ######## LLAVA ########\n",
    "    \n",
    "    generate_llava_outputs(subset_size=10, last_turn_only=False) \n",
    "\n",
    "    # Compute metrics for BLIP responses\n",
    "    llava_pair_stats = pair_stats_by_level(LLAVA_OUTPUT_PATH)\n",
    "    llava_fooling_rate_per_level = fooling_rate_by_level(llava_pair_stats)\n",
    "    llava_acc_per_level = acc_by_level(llava_pair_stats)\n",
    "    \n",
    "    print(\"\\n========================\")\n",
    "    print(\"FOOLING RATE PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in llava_fooling_rate_per_level.items():\n",
    "        print(f\"{lvl}: Fooling Rate = {stats['fooled']}/{stats['total']} \"\n",
    "              f\"({stats['rate']:.2f})\")\n",
    "\n",
    "    print(\"\\n========================\")\n",
    "    print(\"ACCURACY PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in llava_acc_per_level.items():\n",
    "        print(f\"{lvl}:  \"\n",
    "            f\"Acc(correct caption) = {stats['accuracy_correct_caption']:.2f},  \"\n",
    "            f\"Acc(incorrect caption) = {stats['accuracy_incorrect_caption']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5a1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
