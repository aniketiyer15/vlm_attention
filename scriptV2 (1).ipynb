{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"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\"\n",
    "ANTHROPIC_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a452c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"pipeline_images\"           \n",
    "OUTPUT_PATH = \"captions.jsonl\"    \n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a64736",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8379ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8507aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "• Must accurately describe the visible scene.\n",
    "• 7–15 words, objective, simple, and factual.\n",
    "• Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "• Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "• Must keep the same length and sentence structure style as the correct caption.\n",
    "• MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     – species/category of the main object\n",
    "     – color of a main object\n",
    "     – pattern/texture of a main object\n",
    "     – object type that a person is holding/using\n",
    "     – action the main subject is performing\n",
    "     – spatial relation (e.g., “in front of” → “behind”)\n",
    "     \n",
    "• The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, “ocean” is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "• The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "• The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0–L4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 — Pure language prior  \n",
    "• Must be answerable with NO access to the image.  \n",
    "• General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "• 6–14 words.\n",
    "\n",
    "L1 — Probe changed attribute #1 \n",
    "• MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "• Example:If species changed, ask “What type of animal…?”  \n",
    "          If color changed, ask “What color is…?”  \n",
    "          If object type changed, ask “What object is… holding?”  \n",
    "• No attributes other than the first changed one.  \n",
    "• 6–14 words.\n",
    "\n",
    "L2 — Probe changed attribute #2\n",
    "• MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "• Same rules as L2 but targeting the second changed detail.  \n",
    "• Should not be the same question as L1. \n",
    "• 6–14 words.\n",
    "\n",
    "L3 — High-level reasoning\n",
    "• Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "• The question MUST NOT depend on the two changed attributes.\n",
    "• The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "• The question SHOULD require general common-sense or contextual reasoning.\n",
    "• The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "• 6–14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "• Do NOT provide answers.\n",
    "• Do NOT describe the image outside captions.\n",
    "• All questions must be 6–14 words.\n",
    "• Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_blip2(image_path, caption, question, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Runs BLIP-2 Flan-T5-xl on:\n",
    "        IMAGE + (caption + question) text prompt.\n",
    "    Returns: the generated answer as a clean string.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Load image ----\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---- Build prompt for BLIP-2 ----\n",
    "    # Format:  \"<caption>\\n\\nQuestion: <question>\\nAnswer:\"\n",
    "    prompt = f\"{caption}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    # ---- Generate answer ----\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,      # deterministic & stable (recommended)\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # ---- Decode ----\n",
    "    answer = processor.tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # ---- Strip the prompt part (BLIP-2 often echoes input) ----\n",
    "    if \"Answer:\" in answer:\n",
    "        answer = answer.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "    # Clean spacing\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "# \"Salesforce/blip2-flan-t5-xxl\" -> bigger, might need 2x memory\n",
    "MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55894827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ee14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOWER - doesn't parallelize 8 API calls \n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption   = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "#                 L0, L1, L2, L3, L4 = q[\"L0\"], q[\"L1\"], q[\"L2\"], q[\"L3\"], q[\"L4\"]\n",
    "                L0, L1, L2, L3 = q[\"L0\"], q[\"L1\"], q[\"L2\"], q[\"L3\"]\n",
    "\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, correct_caption, L3),\n",
    "#                     \"L4\": ask_blip2(path, correct_caption, L4),\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, incorrect_caption, L3),\n",
    "#                     \"L4\": ask_blip2(path, incorrect_caption, L4),\n",
    "                }\n",
    "\n",
    "                # ---- 3) Write compact JSON ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "#                         \"L4\": L4\n",
    "                    },\n",
    "                    \"answers\": {\n",
    "                        \"correct\": answers_correct,\n",
    "                        \"incorrect\": answers_incorrect\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "#                 Evaluate responses with Claude-3.5 Sonnet\n",
    "                for level, question in output[\"questions\"].items():\n",
    "\n",
    "                    # correct caption condition\n",
    "                    score_c = eval_answer(\n",
    "                        path,\n",
    "                        output[\"captions\"][\"correct\"],\n",
    "                        \"correct caption condition\",\n",
    "                        question,\n",
    "                        output[\"answers\"][\"correct\"][level]\n",
    "                    )\n",
    "\n",
    "                    # incorrect caption condition\n",
    "                    score_i = eval_answer(\n",
    "                        path,\n",
    "                        output[\"captions\"][\"incorrect\"],\n",
    "                        \"incorrect caption condition\",\n",
    "                        question,\n",
    "                        output[\"answers\"][\"incorrect\"][level]\n",
    "                    )\n",
    "\n",
    "                    all_results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"level\": level,\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    })\n",
    "                    \n",
    "                print(all_results)\n",
    "            \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n Error with {image_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTER - Parallelizes API calls\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption   = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, correct_caption, L3),\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, incorrect_caption, L3),\n",
    "                }\n",
    "\n",
    "                # ---- 3) Write compact JSON ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "                    \"answers\": {\n",
    "                        \"correct\": answers_correct,\n",
    "                        \"incorrect\": answers_incorrect\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect\"][level]\n",
    "                        ))\n",
    "\n",
    "                    results = [j.result() for j in as_completed(jobs)]\n",
    "\n",
    "                # results arrive unordered → rebuild matching pairs\n",
    "                # We submitted jobs in pairs, so their order is known\n",
    "                ordered_results = []\n",
    "                for j in jobs:\n",
    "                    ordered_results.append(j.result())\n",
    "\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    all_results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"level\": level,\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n Error with {image_id}: {e}\")\n",
    "\n",
    "    print(all_results)\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa235c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fooling_by_level(jsonl_path):\n",
    "    stats = {\n",
    "        \"L0\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L1\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L2\": {\"fooled\": 0, \"total\": 0},\n",
    "        \"L3\": {\"fooled\": 0, \"total\": 0}\n",
    "    }\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            # ensure consistent level order\n",
    "            for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                scores = eval_scores[level]\n",
    "\n",
    "                s_c = scores[\"correct_caption_score\"]\n",
    "                s_i = scores[\"incorrect_caption_score\"]\n",
    "\n",
    "                stats[level][\"total\"] += 1\n",
    "\n",
    "                # Fooling event = correct under correct caption AND incorrect under incorrect caption\n",
    "                if s_c == 1 and s_i == 0:\n",
    "                    stats[level][\"fooled\"] += 1\n",
    "\n",
    "    # compute fooling rates\n",
    "    for lvl in stats:\n",
    "        total = stats[lvl][\"total\"]\n",
    "        fooled = stats[lvl][\"fooled\"]\n",
    "        stats[lvl][\"rate\"] = fooled / total if total > 0 else 0.0\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run():\n",
    "    image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                # ---- 2) BLIP-2 answers ----\n",
    "                answers_correct = {\n",
    "                    \"L0\": ask_blip2(path, correct_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, correct_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, correct_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, correct_caption, L3)\n",
    "                }\n",
    "\n",
    "                answers_incorrect = {\n",
    "                    \"L0\": ask_blip2(path, incorrect_caption, L0),\n",
    "                    \"L1\": ask_blip2(path, incorrect_caption, L1),\n",
    "                    \"L2\": ask_blip2(path, incorrect_caption, L2),\n",
    "                    \"L3\": ask_blip2(path, incorrect_caption, L3)\n",
    "                }\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"eval_scores\": {}   # will be filled next\n",
    "                }\n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {OUTPUT_PATH}\\n\")\n",
    "    stats = compute_fooling_by_level(OUTPUT_PATH)\n",
    "    print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
