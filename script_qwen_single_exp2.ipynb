{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00947c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56080073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7241e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "QWEN_OUTPUT_PATH = \"qwen_single_exp2_responses.jsonl\" # CHANGE LATER\n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13601d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fa250",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb47f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "============================================================\n",
    "1. Correct Caption\n",
    "============================================================\n",
    "• Accurately describe the visible scene.\n",
    "• 9–15 words, objective, simple, and factual.\n",
    "• Should mention main objects; avoid inference beyond evidence.\n",
    "\n",
    "============================================================\n",
    "2. Visual Necessity Question Ladder (VNL): Levels L0 → L4\n",
    "============================================================\n",
    "\n",
    "GENERAL RULES:\n",
    "• L1–L4 MUST require looking at the image to answer.\n",
    "• All questions MUST be answerable using only the given image.\n",
    "• Do NOT include the answers.\n",
    "• No question should exceed 14 words.\n",
    "• Return concise, natural wording.\n",
    "\n",
    "------------------------------------------------------------\n",
    "L0 – Baseline Question (Language-prior only)\n",
    "------------------------------------------------------------\n",
    "• A question humans can answer **without seeing the image**.\n",
    "• May refer to the world generally (NOT the specific image).\n",
    "• Purpose: control for language-only biases.\n",
    "• 6–12 words.\n",
    "Examples:\n",
    "– “What season often has the coldest weather?”  \n",
    "– “Which animal is larger, a dog or an elephant?”  \n",
    "– “What do people usually use to take photographs?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L1 – Basic Visual Recognition\n",
    "------------------------------------------------------------\n",
    "• Requires the image.\n",
    "• Ask about a **primary object** or its basic property.\n",
    "• No reasoning, no inference.\n",
    "Examples:\n",
    "– “What object is the person holding?”  \n",
    "– “What color is the animal?”  \n",
    "– “How many people are visible?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L2 – Intermediate Visual Detail\n",
    "------------------------------------------------------------\n",
    "• Also requires the image.\n",
    "• Ask about a **secondary property** of a main object.\n",
    "• Slightly more specific than L1.\n",
    "Examples:\n",
    "– “What pattern is on the person’s shirt?”  \n",
    "– “What type of hat is the man wearing?”  \n",
    "– “What material is the table made of?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L3 – Relational / Spatial Reasoning\n",
    "------------------------------------------------------------\n",
    "• Requires image + spatial relations + relational understanding.\n",
    "Examples:\n",
    "– “Where is the dog positioned relative to the child?”  \n",
    "– “What object is behind the bicycle?”  \n",
    "– “Which person is closest to the camera?”\n",
    "\n",
    "------------------------------------------------------------\n",
    "L4 – High-Level Visual Reasoning\n",
    "------------------------------------------------------------\n",
    "• Hardest level; requires the entire scene.\n",
    "• Ask about interactions, goals, implied roles, or multi-object context.\n",
    "• Still must be answerable from the image alone (no external inference).\n",
    "Examples:\n",
    "– “What activity are the people engaged in?”  \n",
    "– “Why is the man extending his arm?”  \n",
    "– “What is the group collectively doing?”\n",
    "\n",
    "============================================================\n",
    "Return EXACTLY this JSON structure:\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",\n",
    "  \"L2\": \"<string>\",\n",
    "  \"L3\": \"<string>\",\n",
    "  \"L4\": \"<string>\"\n",
    "}\n",
    "============================================================\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06363274",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WORKING - but may be wrong!\n",
    "\n",
    "# def compute_qwen_mdi(attns, inputs, image_token_id=151655):\n",
    "#     \"\"\"\n",
    "#     MDI for a single-image Qwen2.5-VL call.\n",
    "\n",
    "#     attns  : list of attention tensors captured from decoder layers\n",
    "#              each with shape (batch, heads, q_len, k_len)\n",
    "#              (we'll gracefully skip anything that isn't this)\n",
    "#     inputs : batch dict that contains \"input_ids\"\n",
    "#     \"\"\"\n",
    "\n",
    "#     import torch\n",
    "\n",
    "#     if attns is None or len(attns) == 0:\n",
    "#         print(\"MDI: no attention tensors\")\n",
    "#         return None\n",
    "\n",
    "#     if \"input_ids\" not in inputs:\n",
    "#         print(\"MDI: inputs missing input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     input_ids = inputs[\"input_ids\"][0]          # (seq_len,)\n",
    "#     img_positions = (input_ids == image_token_id).nonzero().flatten()\n",
    "\n",
    "#     if img_positions.numel() == 0:\n",
    "#         print(\"MDI: no image tokens found in input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     img_start = img_positions[0].item()\n",
    "#     img_end   = img_positions[-1].item() + 1    # non-inclusive\n",
    "\n",
    "#     vision_scores = []\n",
    "#     text_scores = []\n",
    "\n",
    "#     for layer_attn in attns:\n",
    "#         # Some layers may return None or have wrong shape; skip them\n",
    "#         if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "#             continue\n",
    "#         if layer_attn.dim() != 4:\n",
    "#             # e.g. (heads, q, k) -> add batch dim\n",
    "#             if layer_attn.dim() == 3:\n",
    "#                 layer_attn = layer_attn.unsqueeze(0)\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#         # layer_attn: (batch, heads, q_len, k_len)\n",
    "#         attn = layer_attn.mean(dim=1)[0]  # -> (q_len, k_len)\n",
    "\n",
    "#         # safety in case sequence length changed\n",
    "#         q_len, k_len = attn.shape\n",
    "#         if img_end > k_len:\n",
    "#             continue\n",
    "\n",
    "#         vis = attn[:, img_start:img_end].sum().item()\n",
    "#         txt = (attn[:, :img_start].sum() + attn[:, img_end:].sum()).item()\n",
    "\n",
    "#         vision_scores.append(vis)\n",
    "#         text_scores.append(txt)\n",
    "\n",
    "#     if not vision_scores or not text_scores:\n",
    "#         print(\"MDI: no valid layers after filtering\")\n",
    "#         return None\n",
    "\n",
    "#     vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "#     txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "#     mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "#     return float(mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd9a31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WORKING - but may be wrong!\n",
    "\n",
    "# def compute_qwen_mdi(attns, inputs):\n",
    "#     \"\"\"\n",
    "#     Guaranteed-working MDI for Qwen2.5-VL.\n",
    "#     Automatically detects visual tokens using the known Qwen ranges:\n",
    "#       - 151552–151654 : image codebook tokens\n",
    "#       - 151655        : image separator/end token\n",
    "\n",
    "#     MDI = attention_to_visual_tokens / (attention_to_all_other_tokens)\n",
    "#     \"\"\"\n",
    "\n",
    "#     import torch\n",
    "\n",
    "#     if attns is None or len(attns) == 0:\n",
    "#         print(\"MDI: no attn tensors\")\n",
    "#         return None\n",
    "\n",
    "#     if \"input_ids\" not in inputs:\n",
    "#         print(\"MDI: missing input_ids\")\n",
    "#         return None\n",
    "\n",
    "#     input_ids = inputs[\"input_ids\"][0]           # (seq_len,)\n",
    "\n",
    "#     # --- 1. Detect vision tokens robustly ---\n",
    "#     vision_mask = ((input_ids >= 151552) & (input_ids <= 151655))\n",
    "\n",
    "#     visual_positions = vision_mask.nonzero().flatten()\n",
    "#     if visual_positions.numel() == 0:\n",
    "#         print(\"MDI: no visual tokens detected\")\n",
    "#         return None\n",
    "\n",
    "#     v_start = visual_positions[0].item()\n",
    "#     v_end   = visual_positions[-1].item() + 1    # non-inclusive\n",
    "\n",
    "#     vision_scores = []\n",
    "#     text_scores   = []\n",
    "\n",
    "#     # --- 2. Iterate over captured attention ---\n",
    "#     for layer_attn in attns:\n",
    "\n",
    "#         if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "#             continue\n",
    "\n",
    "#         # Accept shapes:\n",
    "#         #   (batch, heads, q_len, k_len)\n",
    "#         #   (heads, q_len, k_len)\n",
    "#         if layer_attn.dim() == 3:\n",
    "#             # add batch dim\n",
    "#             layer_attn = layer_attn.unsqueeze(0)\n",
    "#         elif layer_attn.dim() != 4:\n",
    "#             continue\n",
    "\n",
    "#         # Mean over heads -> (q_len, k_len)\n",
    "#         attn = layer_attn.mean(dim=1)[0]\n",
    "\n",
    "#         q_len, k_len = attn.shape\n",
    "\n",
    "#         # clip if KV cache truncated\n",
    "#         if v_end > k_len:\n",
    "#             continue\n",
    "\n",
    "#         # total attention paid to image tokens\n",
    "#         vis = attn[:, v_start:v_end].sum().item()\n",
    "\n",
    "#         # attention paid to everything else\n",
    "# #         txt = (attn[:, :v_start].sum() + attn[:, v_end:].sum()).item()\n",
    "\n",
    "#         txt_before = attn[:, :v_start].sum().item() if v_start > 0 else 0\n",
    "#         txt_after = attn[:, v_end:].sum().item() if v_end < k_len else 0\n",
    "#         txt = txt_before + txt_after\n",
    "\n",
    "#         vision_scores.append(vis)\n",
    "#         text_scores.append(txt)\n",
    "\n",
    "#     if len(vision_scores) == 0 or len(text_scores) == 0:\n",
    "#         print(\"MDI: no valid layers\")\n",
    "#         return None\n",
    "\n",
    "#     vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "#     txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "#     mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "#     return float(mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working - most likely correct!\n",
    "\n",
    "def compute_qwen_mdi(attns, inputs):\n",
    "    \"\"\"\n",
    "    Guaranteed-correct MDI for Qwen2.5-VL.\n",
    "    Qwen does NOT place real vision tokens in input_ids.\n",
    "    It only inserts repeated <image> placeholder tokens with ID 151655.\n",
    "    The actual image patches stay inside the vision encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    import torch\n",
    "\n",
    "    if attns is None or len(attns) == 0:\n",
    "        return None\n",
    "\n",
    "    if \"input_ids\" not in inputs:\n",
    "        return None\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0]          # (seq_len,)\n",
    "    seq_len = input_ids.shape[0]\n",
    "\n",
    "    # ---- 1. Correct visual span detection ----\n",
    "    # Qwen2.5-VL uses ONLY token_id 151655 as the image placeholder\n",
    "    IMAGE_TOKEN_ID = 151655\n",
    "\n",
    "    visual_positions = (input_ids == IMAGE_TOKEN_ID).nonzero().flatten()\n",
    "    if visual_positions.numel() == 0:\n",
    "        print(\"No 151655 tokens found → Qwen image token missing?\")\n",
    "        return None\n",
    "\n",
    "    v_start = visual_positions[0].item()\n",
    "    v_end   = visual_positions[-1].item() + 1     # non-inclusive\n",
    "\n",
    "    # ---- 2. Accumulate attention ----\n",
    "    vision_scores = []\n",
    "    text_scores = []\n",
    "\n",
    "    for layer_attn in attns:\n",
    "\n",
    "        if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "            continue\n",
    "\n",
    "        # expected (batch, heads, q_len, k_len)\n",
    "        if layer_attn.dim() == 3:\n",
    "            layer_attn = layer_attn.unsqueeze(0)\n",
    "        elif layer_attn.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        attn = layer_attn.mean(dim=1)[0]    # (q_len, k_len)\n",
    "\n",
    "        q_len, k_len = attn.shape\n",
    "        if v_end > k_len:\n",
    "            continue\n",
    "\n",
    "        # vision attention\n",
    "        vis = attn[:, v_start:v_end].sum().item()\n",
    "\n",
    "        # text attention\n",
    "        txt_before = attn[:, :v_start].sum().item()\n",
    "        txt_after  = attn[:, v_end:].sum().item()\n",
    "        txt = txt_before + txt_after\n",
    "\n",
    "        vision_scores.append(vis)\n",
    "        text_scores.append(txt)\n",
    "\n",
    "    if len(vision_scores) == 0:\n",
    "        return None\n",
    "\n",
    "    vis_avg = sum(vision_scores) / len(vision_scores)\n",
    "    txt_avg = sum(text_scores) / len(text_scores)\n",
    "\n",
    "    mdi = vis_avg / (vis_avg + txt_avg + 1e-9)\n",
    "    return float(mdi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a292d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# NOT NORMALIZED \n",
    "\n",
    "def compute_attention_entropy(attns):\n",
    "    \"\"\"\n",
    "    Compute average attention entropy across all Qwen2-VL decoder layers.\n",
    "    \n",
    "    attns : list of attention tensors captured by hooks\n",
    "            each element is (batch, heads, q_len, k_len) or (heads, q_len, k_len)\n",
    "\n",
    "    Returns:\n",
    "        float entropy_score  (lower = more focused, higher = more diffuse)\n",
    "        or None if not computable\n",
    "    \"\"\"\n",
    "\n",
    "    if attns is None or len(attns) == 0:\n",
    "        print(\"Entropy: no attention tensors\")\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in attns:\n",
    "\n",
    "        # Skip invalid entries\n",
    "        if layer_attn is None or not torch.is_tensor(layer_attn):\n",
    "            continue\n",
    "\n",
    "        # Ensure shape is (batch, heads, q, k)\n",
    "        if layer_attn.dim() == 3:\n",
    "            layer_attn = layer_attn.unsqueeze(0)   # (1, heads, q, k)\n",
    "        elif layer_attn.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        # Normalize attention along key dimension\n",
    "        # shape: (batch, heads, q_len, k_len)\n",
    "        attn = layer_attn.float()\n",
    "\n",
    "        # Softmax normalization (just in case the model didn't return normalized attn)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Compute entropy per (batch, head, q)\n",
    "        # H = -sum(p * log(p))\n",
    "        entropy = -(attn * (attn + 1e-12).log()).sum(dim=-1)  # sum over k_len\n",
    "\n",
    "        # Mean over batch, heads, and q positions\n",
    "        entropy = entropy.mean().item()\n",
    "\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    if len(entropies) == 0:\n",
    "        print(\"Entropy: no valid layers\")\n",
    "        return None\n",
    "\n",
    "    # Average entropy across layers\n",
    "    final_entropy = float(sum(entropies) / len(entropies))\n",
    "    return final_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d529e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Version w/ normalization (scaled from 0 to 1)\n",
    "\n",
    "def compute_attention_entropy(attn_maps, mask=None, vision_span=None, normalized=True):\n",
    "    \"\"\"\n",
    "    attn_maps : list of attention tensors\n",
    "        each tensor has shape (batch, heads, q_len, k_len)\n",
    "        or (heads, q_len, k_len)\n",
    "        or (q_len, k_len)\n",
    "\n",
    "    mask : optional boolean mask of shape (k_len,)\n",
    "        True = include that key token\n",
    "        If None, full sequence is used.\n",
    "\n",
    "    vision_span : optional (start, end)\n",
    "        If provided, computes entropy only over this token region.\n",
    "\n",
    "    normalized : bool\n",
    "        If True -> returns H / log(k)\n",
    "        If False -> returns raw entropy.\n",
    "\n",
    "    Returns:\n",
    "        average entropy across layers + heads as float\n",
    "    \"\"\"\n",
    "    if attn_maps is None or len(attn_maps) == 0:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer_attn in attn_maps:\n",
    "\n",
    "        # ---- 1. Fix shapes ----\n",
    "        if layer_attn.dim() == 3:       # (heads, q, k)\n",
    "            layer_attn = layer_attn.unsqueeze(0)  # -> (1, heads, q, k)\n",
    "        elif layer_attn.dim() == 2:     # (q, k)\n",
    "            layer_attn = layer_attn.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        if layer_attn.dim() != 4:\n",
    "            continue  # bad layer\n",
    "\n",
    "        b, h, q_len, k_len = layer_attn.shape\n",
    "\n",
    "        # ---- 2. Select region (vision / mask) ----\n",
    "        attn = layer_attn  # (b, heads, q, k)\n",
    "\n",
    "        if vision_span is not None:\n",
    "            start, end = vision_span\n",
    "            attn = attn[:, :, :, start:end]\n",
    "            k_len = end - start\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: (k,)\n",
    "            mask = mask.to(attn.device)\n",
    "            attn = attn[:, :, :, mask]\n",
    "            k_len = mask.sum().item()\n",
    "\n",
    "        if k_len <= 1:\n",
    "            continue\n",
    "\n",
    "        # ---- 3. Normalize probabilities (safety) ----\n",
    "        attn = attn.clamp(min=1e-9)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # ---- 4. Entropy ----\n",
    "        H = -(attn * attn.log()).sum(dim=-1)  # (b, heads, q)\n",
    "        H = H.mean().item()  # average everything\n",
    "\n",
    "        # ---- 5. Normalize 0–1 if requested ----\n",
    "        if normalized:\n",
    "            H = H / math.log(k_len)\n",
    "\n",
    "        entropies.append(H)\n",
    "\n",
    "    if len(entropies) == 0:\n",
    "        return None\n",
    "\n",
    "    return float(sum(entropies) / len(entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce57b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WRONG - but keeping just in case\n",
    "\n",
    "# def compute_attention_shift(attns_A, attns_B, vision_span=None, normalized=True):\n",
    "#     \"\"\"\n",
    "#     Computes attention shift between two model runs (A and B).\n",
    "    \n",
    "#     Each attns_X is a list of attention tensors:\n",
    "#        (batch, heads, q_len, k_len) or (heads, q_len, k_len) or (q_len, k_len).\n",
    "\n",
    "#     vision_span: (start, end)  → compute shift only over visual tokens\n",
    "#     normalized: normalize final shift to [0, 1]\n",
    "\n",
    "#     Returns:\n",
    "#         scalar attention-shift score\n",
    "#     \"\"\"\n",
    "\n",
    "#     if attns_A is None or attns_B is None:\n",
    "#         print(\"Shift: missing attn maps\")\n",
    "#         return None\n",
    "\n",
    "#     if len(attns_A) == 0 or len(attns_B) == 0:\n",
    "#         print(\"Shift: empty attn lists\")\n",
    "#         return None\n",
    "\n",
    "#     shifts = []\n",
    "\n",
    "#     # Iterate layer-by-layer (stop at min length)\n",
    "#     L = min(len(attns_A), len(attns_B))\n",
    "\n",
    "#     for i in range(L):\n",
    "#         A = attns_A[i]\n",
    "#         B = attns_B[i]\n",
    "\n",
    "#         # Ensure both are valid tensors\n",
    "#         if A is None or B is None:\n",
    "#             continue\n",
    "#         if not (torch.is_tensor(A) and torch.is_tensor(B)):\n",
    "#             continue\n",
    "\n",
    "#         # ---- 1. Normalize shapes to (1, heads, q, k) ----\n",
    "#         def fix_shape(X):\n",
    "#             if X.dim() == 2:      # (q, k)\n",
    "#                 return X.unsqueeze(0).unsqueeze(0)\n",
    "#             if X.dim() == 3:      # (heads, q, k)\n",
    "#                 return X.unsqueeze(0)\n",
    "#             return X  # assume (batch, heads, q, k)\n",
    "        \n",
    "#         A = fix_shape(A)\n",
    "#         B = fix_shape(B)\n",
    "\n",
    "#         # ---- 2. Align sequence lengths ----\n",
    "#         _, hA, qA, kA = A.shape\n",
    "#         _, hB, qB, kB = B.shape\n",
    "\n",
    "#         q = min(qA, qB)\n",
    "#         k = min(kA, kB)\n",
    "\n",
    "#         A = A[:, :, :q, :k]\n",
    "#         B = B[:, :, :q, :k]\n",
    "\n",
    "#         # ---- 3. Optional: restrict to vision tokens ----\n",
    "#         if vision_span is not None:\n",
    "#             v_start, v_end = vision_span\n",
    "#             v_end = min(v_end, k)\n",
    "#             A = A[:, :, :, v_start:v_end]\n",
    "#             B = B[:, :, :, v_start:v_end]\n",
    "\n",
    "#         # ---- 4. Normalize to probability distributions ----\n",
    "#         A = A.clamp(min=1e-9)\n",
    "#         B = B.clamp(min=1e-9)\n",
    "\n",
    "#         A = A / A.sum(dim=-1, keepdim=True)\n",
    "#         B = B / B.sum(dim=-1, keepdim=True)\n",
    "\n",
    "#         # ---- 5. L1 distance (attention shift) ----\n",
    "#         # average across batch, heads, and queries\n",
    "#         shift_val = torch.abs(A - B).sum(dim=-1).mean().item()\n",
    "\n",
    "#         shifts.append(shift_val)\n",
    "\n",
    "#     if len(shifts) == 0:\n",
    "#         print(\"Shift: no valid layers after processing\")\n",
    "#         return None\n",
    "\n",
    "#     shift_raw = sum(shifts) / len(shifts)\n",
    "\n",
    "#     # ---- 6. Normalize shift to 0–1 scale ----\n",
    "#     # Maximum L1 between two distributions is 2\n",
    "#     if normalized:\n",
    "#         shift_raw = shift_raw / 2.0  \n",
    "\n",
    "#     return float(shift_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention_shift(prev_attns, curr_attns, inputs):\n",
    "    \"\"\"\n",
    "    Computes attention shift between two levels for Qwen2.5-VL.\n",
    "    Uses only LAST COMMON QUERY token to avoid shape mismatch.\n",
    "    \"\"\"\n",
    "\n",
    "    if prev_attns is None or curr_attns is None:\n",
    "        return None\n",
    "    if len(prev_attns) == 0 or len(curr_attns) == 0:\n",
    "        return None\n",
    "\n",
    "    # ===== 1. Find visual region using Qwen input_ids =====\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    visual_mask = ((input_ids >= 151552) & (input_ids <= 151655))\n",
    "    visual_positions = visual_mask.nonzero().flatten()\n",
    "\n",
    "    if visual_positions.numel() == 0:\n",
    "        print(\"Shift: no visual tokens detected in input_ids\")\n",
    "        return None\n",
    "\n",
    "    v_start = visual_positions[0].item()\n",
    "    v_end   = visual_positions[-1].item() + 1\n",
    "\n",
    "    layer_shifts = []\n",
    "\n",
    "    # ===== 2. Layer-by-layer shift =====\n",
    "    for A_prev, A_curr in zip(prev_attns, curr_attns):\n",
    "\n",
    "        if A_prev is None or A_curr is None:\n",
    "            continue\n",
    "        if not (torch.is_tensor(A_prev) and torch.is_tensor(A_curr)):\n",
    "            continue\n",
    "\n",
    "        # unify shapes\n",
    "        if A_prev.dim() == 3: A_prev = A_prev.unsqueeze(0)\n",
    "        if A_curr.dim() == 3: A_curr = A_curr.unsqueeze(0)\n",
    "        if A_prev.dim() != 4 or A_curr.dim() != 4:\n",
    "            continue\n",
    "\n",
    "        # mean over heads → (q_len, k_len)\n",
    "        A_prev = A_prev.mean(dim=1)[0]\n",
    "        A_curr = A_curr.mean(dim=1)[0]\n",
    "\n",
    "        q_len_prev, k_len_prev = A_prev.shape\n",
    "        q_len_curr, k_len_curr = A_curr.shape\n",
    "\n",
    "        # align key dimension\n",
    "        k_len = min(k_len_prev, k_len_curr)\n",
    "        A_prev = A_prev[:, :k_len]\n",
    "        A_curr = A_curr[:, :k_len]\n",
    "\n",
    "        # visual slice must be valid\n",
    "        if v_end > k_len:\n",
    "            continue\n",
    "\n",
    "        # ===== *** FIX: align QUERY dimension *** =====\n",
    "        q_len = min(q_len_prev, q_len_curr)\n",
    "        # pick last common query\n",
    "        A_prev_last = A_prev[q_len - 1, v_start:v_end]\n",
    "        A_curr_last = A_curr[q_len - 1, v_start:v_end]\n",
    "\n",
    "        # normalize\n",
    "        A_prev_last = A_prev_last.clamp(1e-9)\n",
    "        A_curr_last = A_curr_last.clamp(1e-9)\n",
    "        A_prev_last = A_prev_last / A_prev_last.sum()\n",
    "        A_curr_last = A_curr_last / A_curr_last.sum()\n",
    "\n",
    "        # cosine distance\n",
    "        cos_sim = F.cosine_similarity(\n",
    "            A_prev_last, A_curr_last, dim=0\n",
    "        )\n",
    "        shift = float(1 - cos_sim.clamp(-1, 1))\n",
    "        layer_shifts.append(shift)\n",
    "\n",
    "    if len(layer_shifts) == 0:\n",
    "        return None\n",
    "\n",
    "    return sum(layer_shifts) / len(layer_shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742988b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ask_qwen(\n",
    "    image_path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_metrics=True):\n",
    "    \"\"\"\n",
    "    Runs Qwen-VL with image + (caption + question) text prompt.\n",
    "    Supports:\n",
    "        - returning answer only\n",
    "        - returning answer + MDI\n",
    "        - returning answer + MDI + attention tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 0. Initialize ----------\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # ---------- 1. Load image ----------\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # ---------- 2. Build fixed-format prompt ----------\n",
    "    answer_rules = (\n",
    "        \"Please answer in plain text only.\\n\"\n",
    "        \"Do NOT use markdown formatting.\\n\"\n",
    "        \"Keep the answer short (1–2 sentences).\\n\"\n",
    "        \"Provide only the direct answer without any explanation.\"\n",
    "    )\n",
    "\n",
    "    # ---------- 3. Build message list in the CORRECT, SAFE ORDER ----------\n",
    "    messages = []\n",
    "\n",
    "#     # (A) Add history FIRST (chronological)\n",
    "#     for q_prev, a_prev in history:\n",
    "#         messages.append({\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"text\", \"text\": q_prev}\n",
    "#             ]\n",
    "#         })\n",
    "#         messages.append({\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"text\", \"text\": a_prev}\n",
    "#             ]\n",
    "#         })\n",
    "\n",
    "    # (B) Add CURRENT TURN LAST\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # 1. QUESTION FIRST — prevents L0 contamination\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "\n",
    "            # 2. IMAGE SECOND\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "\n",
    "            # 3. CAPTION THIRD — labeled to avoid mixing with instructions\n",
    "            {\"type\": \"text\", \"text\": f\"Caption: {caption}\"},\n",
    "\n",
    "            # 4. ANSWER RULES LAST — separate block\n",
    "            {\"type\": \"text\", \"text\": answer_rules}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # ---------- 4. Preprocess ----------\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # ---------- 5. Vision preprocess ----------\n",
    "    try:\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "    except ImportError:\n",
    "        # fallback\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "    # IMPORTANT for attention extraction\n",
    "    model.config.use_cache = False\n",
    "\n",
    "#     # ---------- 6. Generate with attention extraction ----------\n",
    "    \n",
    "    \n",
    "    def find_decoder_self_attn_layers(model):\n",
    "        layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if name.endswith(\"self_attn\") and \"language_model\" in name:\n",
    "                layers.append(module)\n",
    "        return layers\n",
    "    \n",
    "    collected_attns = []\n",
    "\n",
    "    def save_attn(module, inp, out):\n",
    "        attn = out[1]  # (batch, heads, q_len, k_len)\n",
    "        collected_attns.append(attn.detach().cpu())\n",
    "\n",
    "    layers = find_decoder_self_attn_layers(model)\n",
    "    \n",
    "    hooks = []\n",
    "    for layer in layers:\n",
    "        h = layer.register_forward_hook(save_attn)\n",
    "        hooks.append(h)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        \n",
    "    # --- REMOVE HOOKS ---\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "\n",
    "    # ---------- 7. Decode answer ----------\n",
    "    generated_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]  # remove prompt tokens\n",
    "    answer = processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Remove possible \"ASSISTANT:\" prefixes\n",
    "    if \"ASSISTANT:\" in answer:\n",
    "        answer = answer.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "    # ---------- 8. Metrics ----------\n",
    "    if return_metrics:\n",
    "        try:\n",
    "            final_mdi = compute_qwen_mdi(collected_attns, inputs)\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "        except:\n",
    "            print(\"fail\")\n",
    "            final_mdi = None\n",
    "\n",
    "        return answer, final_mdi, collected_attns, inputs # INPUTS ADDED FOR ATTENTION SHIFT CALC\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd587331",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QWEN MODEL\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "print(\"Loading Qwen model...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "print(\"✅ Qwen Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_qwen_outputs(subset_size=None, last_turn_only=False):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(QWEN_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "                \n",
    "                # ---- CLEAN CAPTIONS HERE ----\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "                L4 = q[\"L4\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "\n",
    "                prev_attn = None\n",
    "        \n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3), (\"L4\", L4)]:\n",
    "#                     ans, mdi, attn = ask_llava(path, correct_caption, q, return_mdi=True, return_attn=True)\n",
    "                    ans, mdi, attn, inputs = ask_qwen(path, correct_caption, q, return_metrics=True)\n",
    "                            \n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    #entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn, inputs)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"caption\": correct_caption,\n",
    "             \n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3,\n",
    "                        \"L4\": L4\n",
    "                    },\n",
    "\n",
    "                    \"answers\": answers_correct,\n",
    "                    \n",
    "                    \"metrics\": {},\n",
    "                    \n",
    "                    \"eval_scores\": {} \n",
    "                }\n",
    "                \n",
    "                levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "                \n",
    "                for lvl in levels:\n",
    "                    output[\"metrics\"][lvl] = {\n",
    "                    \"mdi\": mdi_correct.get(lvl),\n",
    "                    \"entropy\": entropy_correct.get(lvl),\n",
    "                    \"shift\": shift_correct.get(lvl)}\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"caption\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for lvl in levels:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    output[\"eval_scores\"][lvl] = score_c\n",
    "                    \n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {QWEN_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ############## QWEN ##############\n",
    "        \n",
    "    generate_qwen_outputs(subset_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\"]\n",
    "\n",
    "# accumulators\n",
    "sum_mdi = {lvl: 0.0 for lvl in levels}\n",
    "sum_entropy = {lvl: 0.0 for lvl in levels}\n",
    "sum_shift = {lvl: 0.0 for lvl in levels}\n",
    "sum_acc = {lvl: 0.0 for lvl in levels}\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(QWEN_OUTPUT_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        metrics = entry[\"metrics\"]\n",
    "        evals = entry[\"eval_scores\"]\n",
    "        \n",
    "        for lvl in levels:\n",
    "            sum_mdi[lvl] += metrics[lvl][\"mdi\"]\n",
    "            sum_entropy[lvl] += metrics[lvl][\"entropy\"]\n",
    "            # skip null shift on L0\n",
    "            if metrics[lvl][\"shift\"] is not None:\n",
    "                sum_shift[lvl] += metrics[lvl][\"shift\"]\n",
    "            sum_acc[lvl] += evals[lvl]\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "# compute averages\n",
    "avg_mdi = {lvl: sum_mdi[lvl] / count for lvl in levels}\n",
    "avg_entropy = {lvl: sum_entropy[lvl] / count for lvl in levels}\n",
    "avg_shift = {lvl: (sum_shift[lvl] / count if lvl != \"L0\" else None) for lvl in levels}\n",
    "avg_acc = {lvl: sum_acc[lvl] / count for lvl in levels}\n",
    "\n",
    "print(\"Average MDI:\", avg_mdi)\n",
    "print(\"Average Entropy:\", avg_entropy)\n",
    "print(\"Average Shift:\", avg_shift)\n",
    "print(\"Average Accuracy:\", avg_acc)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
