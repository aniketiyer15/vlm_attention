{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887389",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE - replace GT username below ######\n",
    "%cd /home/hice1/nbalakrishna3/scratch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\" OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd312c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"datasets/coco/images/train2017\"          \n",
    "BLIP_OUTPUT_PATH = \"blip_multi_exp1_responses.jsonl\"  \n",
    "GPT_MODEL = \"gpt-4.1-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_OUTPUT = 200               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d4023",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "print(anthropic_client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa6562",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(base64_image):\n",
    "    prompt = \"\"\"\n",
    "You are preparing controlled experimental materials for multimodal evaluation.\n",
    "\n",
    "Given the IMAGE (provided separately), generate the following:\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "1. A correct caption\n",
    "----------------------------------------------------------------------\n",
    "• Must accurately describe the visible scene.\n",
    "• 7–15 words, objective, simple, and factual.\n",
    "• Must mention the main subject(s) and one key attribute\n",
    "  (e.g., species, color, object type, action, or spatial relation).\n",
    "• Should be worded well and clearly. \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "2. A deliberately incorrect caption\n",
    "----------------------------------------------------------------------\n",
    "• Must keep the same length and sentence structure style as the correct caption.\n",
    "• MUST change EXACTLY TWO meaningful visual attributes from the correct caption.\n",
    "  Allowed attribute types:\n",
    "     – species/category of the main object\n",
    "     – color of a main object\n",
    "     – pattern/texture of a main object\n",
    "     – object type that a person is holding/using\n",
    "     – action the main subject is performing\n",
    "     – spatial relation (e.g., “in front of” → “behind”)\n",
    "     \n",
    "• The incorrect caption MUST be **factually wrong for THIS image**.\n",
    "  It should contradict TWO concrete visual facts visible in the picture, not merely\n",
    "  describe an alternative plausible real-world scenario -- VERY IMPORTANT!\n",
    "  (Example: If the scene shows a lake, “ocean” is *not* allowed because both can\n",
    "   coexist conceptually; the changed attributes must be unambiguously false.)\n",
    "\n",
    "• The incorrect caption must remain syntactically valid and plausible for the \n",
    "  kind of world the image depicts, but factually wrong.\n",
    "\n",
    "• The two changed attributes MUST be *the most visually important attributes*\n",
    "  from the correct caption.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. Five Visual Necessity Ladder (VNL) questions (L0–L4)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "L0 — Pure language prior  \n",
    "• Must be answerable with NO access to the image.  \n",
    "• General world knowledge only; do NOT reference animals, people,\n",
    "  objects, nature, or environments.  \n",
    "• 6–14 words.\n",
    "\n",
    "L1 — Probe changed attribute #1 \n",
    "• MUST directly probe the FIRST changed attribute from the incorrect caption.  \n",
    "• Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "• Example:If species changed, ask “What type of animal…?”  \n",
    "          If color changed, ask “What color is…?”  \n",
    "          If object type changed, ask “What object is… holding?”  \n",
    "• No attributes other than the first changed one.  \n",
    "• 6–14 words.\n",
    "\n",
    "L2 — Probe changed attribute #2\n",
    "• MUST directly probe the SECOND changed attribute from the incorrect caption.  \n",
    "• Do NOT explicitly mention the changed attribute in the question (may reference attribute category though). \n",
    "• Same rules as L2 but targeting the second changed detail.  \n",
    "• Should not be the same question as L1. \n",
    "• 6–14 words.\n",
    "\n",
    "L3 — High-level reasoning\n",
    "• Ask a reasoning question that is loosely related to the scene shown in the image.\n",
    "• The question MUST NOT depend on the two changed attributes.\n",
    "• The question MUST NOT target the same object/attribute as L1 or L2.\n",
    "• The question SHOULD require general common-sense or contextual reasoning.\n",
    "• The question SHOULD still be answerable using the image (but only its general context, not the altered details).\n",
    "• 6–14 words.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "GENERAL RULES\n",
    "----------------------------------------------------------------------\n",
    "• Do NOT provide answers.\n",
    "• Do NOT describe the image outside captions.\n",
    "• All questions must be 6–14 words.\n",
    "• Output MUST be a single JSON object in the exact format below.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Return EXACTLY this JSON structure:\n",
    "----------------------------------------------------------------------\n",
    "{\n",
    "  \"correct_caption\": \"<string>\",\n",
    "  \"incorrect_caption\": \"<string>\",\n",
    "  \"L0\": \"<string>\",\n",
    "  \"L1\": \"<string>\",    // targets changed attribute #1\n",
    "  \"L2\": \"<string>\",    // targets changed attribute #2\n",
    "  \"L3\": \"<string>\"   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        model=GPT_MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 ADDED - in version to compute attention-based MDI \n",
    "\n",
    "def compute_mdi(attentions, n_img_tokens=32):\n",
    "    \"\"\"\n",
    "    Compute Modality Dominance Index (MDI) from BLIP-2 cross-attention tensors.\n",
    "\n",
    "    attentions:\n",
    "        - tuple of (num_layers,) where each element is either:\n",
    "              Tensor[batch, heads, tgt_len, src_len]\n",
    "          OR nested tuples depending on HF version.\n",
    "    n_img_tokens: number of image tokens in the cross-attention source sequence.\n",
    "\n",
    "    Returns:\n",
    "        Single float MDI value:\n",
    "            visual_attn / (visual_attn + textual_attn + 1e-9)\n",
    "        or None if no valid attention tensors were found.\n",
    "    \"\"\"\n",
    "\n",
    "    flat_attns = []\n",
    "\n",
    "    # ---- 1. Flatten nested tuples ----\n",
    "    if isinstance(attentions, (list, tuple)):\n",
    "        for a in attentions:\n",
    "            if isinstance(a, (list, tuple)):\n",
    "                flat_attns.extend([\n",
    "                    x for x in a if isinstance(x, torch.Tensor)\n",
    "                ])\n",
    "            elif isinstance(a, torch.Tensor):\n",
    "                flat_attns.append(a)\n",
    "    elif isinstance(attentions, torch.Tensor):\n",
    "        flat_attns.append(attentions)\n",
    "\n",
    "    if not flat_attns:\n",
    "        print(\"⚠️  No attention tensors found.\")\n",
    "        return None\n",
    "\n",
    "    visual_scores = []\n",
    "    textual_scores = []\n",
    "\n",
    "    # ---- 2. Compute visual/textual attention for each layer ----\n",
    "    for layer_attn in flat_attns:\n",
    "        if not isinstance(layer_attn, torch.Tensor):\n",
    "            continue\n",
    "\n",
    "        # layer_attn shape: [batch, heads, tgt_len, src_len]\n",
    "        # average over batch + heads\n",
    "        attn_mean = layer_attn.mean(dim=(0, 1))  # -> [tgt_len, src_len]\n",
    "\n",
    "        tgt_len, src_len = attn_mean.shape\n",
    "\n",
    "        # safety check\n",
    "        n_img_tokens_safe = min(n_img_tokens, src_len)\n",
    "\n",
    "        # first n tokens = image tokens\n",
    "        visual = attn_mean[:, :n_img_tokens_safe].mean().item()\n",
    "\n",
    "        # rest = text tokens\n",
    "        textual = attn_mean[:, n_img_tokens_safe:].mean().item()\n",
    "\n",
    "        visual_scores.append(visual)\n",
    "        textual_scores.append(textual)\n",
    "\n",
    "    if not visual_scores:\n",
    "        return None\n",
    "\n",
    "    # Average over layers\n",
    "    visual_avg = sum(visual_scores) / len(visual_scores)\n",
    "    textual_avg = sum(textual_scores) / len(textual_scores)\n",
    "\n",
    "    # ---- 3. Modality Dominance Index ----\n",
    "    mdi = visual_avg / (visual_avg + textual_avg + 1e-9)\n",
    "\n",
    "    return mdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 (Only BLIP compatibility)\n",
    "\n",
    "def flatten_attn_tensors(attentions):\n",
    "    \"\"\"\n",
    "    Flattens nested BLIP-2 attention structures into a simple list of tensors.\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "\n",
    "    if isinstance(attentions, (list, tuple)):\n",
    "        for a in attentions:\n",
    "            if isinstance(a, (list, tuple)):\n",
    "                flat.extend([x for x in a if isinstance(x, torch.Tensor)])\n",
    "            elif isinstance(a, torch.Tensor):\n",
    "                flat.append(a)\n",
    "    elif isinstance(attentions, torch.Tensor):\n",
    "        flat.append(attentions)\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "#V1 (Only BLIP compatibility)\n",
    "\n",
    "def compute_attention_entropy(attentions):\n",
    "    \"\"\"\n",
    "    Compute entropy of BLIP-2 decoder/cross-attention tensors.\n",
    "    \"\"\"\n",
    "    flat = flatten_attn_tensors(attentions)\n",
    "    if not flat:\n",
    "        return None\n",
    "\n",
    "    entropies = []\n",
    "\n",
    "    for layer in flat:\n",
    "        if not isinstance(layer, torch.Tensor):\n",
    "            continue\n",
    "        if layer.numel() == 0:     # <-- important fix\n",
    "            continue\n",
    "\n",
    "        # Normalize to [batch, heads, tgt, src]\n",
    "        if layer.dim() == 3:\n",
    "            layer = layer.unsqueeze(0)\n",
    "\n",
    "        logits = layer.float()                # [batch, heads, tgt, src]\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        probs  = probs.clamp(min=1e-9)\n",
    "\n",
    "        entropy = -(probs * probs.log()).sum(dim=-1)   # [batch, heads, tgt]\n",
    "        entropies.append(entropy.mean().item())        # scalar\n",
    "\n",
    "    return sum(entropies) / len(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c056687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pad the src_len dimension so that 'a' and 'b' have the same shape.\n",
    "    Padding is applied on the last dimension (src_len).\n",
    "    Shapes expected: [batch, heads, tgt_len, src_len].\n",
    "    \"\"\"\n",
    "    if a.size(-1) == b.size(-1):\n",
    "        return a, b\n",
    "\n",
    "    diff = a.size(-1) - b.size(-1)\n",
    "\n",
    "    if diff > 0:\n",
    "        # a is longer — pad b\n",
    "        pad = (0, diff)  # pad right side of src_len\n",
    "        b = torch.nn.functional.pad(b, pad)\n",
    "    else:\n",
    "        # b is longer — pad a\n",
    "        pad = (0, -diff)\n",
    "        a = torch.nn.functional.pad(a, pad)\n",
    "\n",
    "    return a, b\n",
    "\n",
    "#V1 (Only BLIP compatibility)\n",
    "\n",
    "def compute_attention_shift(prev_attn, curr_attn):\n",
    "    \"\"\"\n",
    "    Compute average L1 shift between two sets of attention tensors.\n",
    "    Handles nested tuples, 3D/4D mismatches, and differing src_len.\n",
    "    \"\"\"\n",
    "    prev_flat = flatten_attn_tensors(prev_attn)\n",
    "    curr_flat = flatten_attn_tensors(curr_attn)\n",
    "\n",
    "    shifts = []\n",
    "\n",
    "    for A, B in zip(prev_flat, curr_flat):\n",
    "        if not (isinstance(A, torch.Tensor) and isinstance(B, torch.Tensor)):\n",
    "            continue\n",
    "        if A.numel() == 0 or B.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Normalize 3D → 4D: [heads, tgt, src] → [1, heads, tgt, src]\n",
    "        if A.dim() == 3:\n",
    "            A = A.unsqueeze(0)\n",
    "        if B.dim() == 3:\n",
    "            B = B.unsqueeze(0)\n",
    "\n",
    "        # src_len might differ → pad\n",
    "        A, B = pad_to_match(A, B)\n",
    "\n",
    "        # Compute shift: mean absolute difference\n",
    "        shift = torch.abs(A - B).mean().item()\n",
    "        shifts.append(shift)\n",
    "\n",
    "    if not shifts:\n",
    "        return None\n",
    "\n",
    "    return sum(shifts) / len(shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccae75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 - working - but might contain an error in the prompt\n",
    "\n",
    "def ask_blip2(\n",
    "    path, \n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_mdi=False,\n",
    "    return_attn=False\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # ===== Build MULTITURN prompt =====\n",
    "    prompt_parts = []\n",
    "\n",
    "    # Caption always appears at top\n",
    "    prompt_parts.append(caption)\n",
    "    prompt_parts.append(\"\\n\")\n",
    "\n",
    "    # Append previous Q/A pairs\n",
    "    if len(history) > 0:\n",
    "        prompt_parts.append(\"Previous conversation:\\n\")\n",
    "        for i, (q_prev, a_prev) in enumerate(history):\n",
    "            prompt_parts.append(f\"Q{i}: {q_prev}\\nA{i}: {a_prev}\\n\")\n",
    "        prompt_parts.append(\"\\n\")\n",
    "\n",
    "    # Current question\n",
    "    prompt_parts.append(\"Current question:\\n\")\n",
    "    prompt_parts.append(f\"Q: {question}\\nA:\")\n",
    "\n",
    "    prompt = \"\".join(prompt_parts)\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, torch.float16)\n",
    "\n",
    "    # FAST PATH (no MDI)\n",
    "    if not return_mdi and not return_attn:\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "        answer = processor.tokenizer.decode(\n",
    "            output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        if \"A:\" in answer:\n",
    "            answer = answer.split(\"A:\")[-1].strip()\n",
    "\n",
    "        return answer\n",
    "\n",
    "    # SLOW PATH (get attentions)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    answer = processor.tokenizer.decode(\n",
    "        outputs.sequences[0], skip_special_tokens=True\n",
    "    )\n",
    "    if \"A:\" in answer:\n",
    "        answer = answer.split(\"A:\")[-1].strip()\n",
    "\n",
    "    # Get attention tensors\n",
    "    attns = (\n",
    "        outputs.cross_attentions\n",
    "        if hasattr(outputs, \"cross_attentions\") and outputs.cross_attentions\n",
    "        else outputs.decoder_attentions\n",
    "    )\n",
    "\n",
    "    # Compute MDI if needed\n",
    "    if return_mdi:\n",
    "        mdi = compute_mdi(attns)\n",
    "        if return_attn:\n",
    "            return answer, mdi, attns\n",
    "        return answer, mdi\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Previous Turns\n",
    "\n",
    "def ask_blip2(\n",
    "    path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_mdi=False,\n",
    "    return_attn=False\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # ===== Build BLIP-2-Compatible Prompt =====\n",
    "    prompt_parts = []\n",
    "\n",
    "    # Caption\n",
    "    prompt_parts.append(f\"Caption: {caption}\\n\\n\")\n",
    "\n",
    "    # Only use the *last history turn*\n",
    "    if len(history) > 0:\n",
    "        q_prev, a_prev = history[-1]\n",
    "        prompt_parts.append(\"Previous QA:\\n\")\n",
    "        prompt_parts.append(f\"Question: {q_prev}\\n\")\n",
    "        prompt_parts.append(f\"Answer: {a_prev}\\n\\n\")\n",
    "\n",
    "    # Current question\n",
    "    prompt_parts.append(f\"Question: {question}\\nAnswer: \")\n",
    "\n",
    "    prompt = \"\".join(prompt_parts)\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, torch.float16)\n",
    "\n",
    "    # FAST PATH (no attentions)\n",
    "    if not return_mdi and not return_attn:\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "        answer = processor.tokenizer.decode(\n",
    "            output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer after \"Answer:\"\n",
    "        if \"Answer:\" in answer:\n",
    "            answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "        return answer\n",
    "\n",
    "    # ---- SLOW PATH (with attentions) ----\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True\n",
    "    )\n",
    "\n",
    "    answer = processor.tokenizer.decode(\n",
    "        outputs.sequences[0], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    if \"Answer:\" in answer:\n",
    "        answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Extract attentions\n",
    "    attns = (\n",
    "        outputs.cross_attentions\n",
    "        if hasattr(outputs, \"cross_attentions\") and outputs.cross_attentions\n",
    "        else outputs.decoder_attentions\n",
    "    )\n",
    "\n",
    "    if return_mdi:\n",
    "        mdi = compute_mdi(attns)\n",
    "        if return_attn:\n",
    "            return answer, mdi, attns\n",
    "        return answer, mdi\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa02f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Turn\n",
    "\n",
    "def ask_blip2(\n",
    "    path,\n",
    "    caption,\n",
    "    question,\n",
    "    history=None,\n",
    "    max_new_tokens=50,\n",
    "    return_mdi=False,\n",
    "    return_attn=False\n",
    "):\n",
    "    # Ensure history is a list\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # ===== Build BLIP-2-Compatible Prompt =====\n",
    "    prompt_parts = []\n",
    "\n",
    "    # Caption at top\n",
    "    prompt_parts.append(f\"Caption: {caption}\\n\\n\")\n",
    "\n",
    "    # Only include *last* turn of history\n",
    "    if len(history) > 0:\n",
    "        q_prev, a_prev = history[-1]\n",
    "        prompt_parts.append(\"Previous QA:\\n\")\n",
    "        prompt_parts.append(f\"Question: {q_prev}\\n\")\n",
    "        prompt_parts.append(f\"Answer: {a_prev}\\n\\n\")\n",
    "\n",
    "    # Current turn\n",
    "    prompt_parts.append(f\"Question: {question}\\n\")\n",
    "    prompt_parts.append(\"Answer: \")\n",
    "\n",
    "    # Build final prompt string\n",
    "    prompt = \"\".join(prompt_parts)\n",
    "\n",
    "    # Preprocess with BLIP processor\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        prompt,\n",
    "        return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "    # === FAST PATH (no attention tracking) ===\n",
    "    if not return_mdi and not return_attn:\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "        answer = processor.tokenizer.decode(\n",
    "            output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer after \"Answer:\"\n",
    "        if \"Answer:\" in answer:\n",
    "            answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "        return answer\n",
    "\n",
    "    # === SLOW PATH (with attention maps) ===\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    answer = processor.tokenizer.decode(\n",
    "        outputs.sequences[0], skip_special_tokens=True\n",
    "    )\n",
    "    if \"Answer:\" in answer:\n",
    "        answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Extract attention tensors\n",
    "    attns = (\n",
    "        outputs.cross_attentions\n",
    "        if hasattr(outputs, \"cross_attentions\") and outputs.cross_attentions\n",
    "        else outputs.decoder_attentions\n",
    "    )\n",
    "\n",
    "    # Compute MDI\n",
    "    if return_mdi:\n",
    "        mdi = compute_mdi(attns)\n",
    "        if return_attn:\n",
    "            return answer, mdi, attns\n",
    "        return answer, mdi\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f287e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR PACE ICE ONLY - replace GT username below ######\n",
    "\n",
    "# Tells HuggingFace to save all downloaded models + datasets in scratch directory instead of home directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/nbalakrishna3/scratch/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/nbalakrishna3/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d692e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor, AutoConfig\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "# ---- 1. Load config and enable attentions ----\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.output_attentions = True\n",
    "config.return_dict = True\n",
    "config.return_dict_in_generate = True\n",
    "\n",
    "# ---- 2. Load processor normally ----\n",
    "print(\"Loading BLIP model...\")\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ---- 3. Load model with modified config ----\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (with attentions enabled)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ea6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(caption, condition, question, model_answer):\n",
    "    return f\"\"\"\n",
    "You are an automated scoring module. You MUST output ONLY a single character: \"0\" or \"1\".\n",
    "NEVER output words, sentences, explanations, punctuation, or reasoning.\n",
    "\n",
    "YOUR TASK:\n",
    "Judge whether the MODEL_ANSWER correctly matches what is visually true in the IMAGE.\n",
    "\n",
    "IMPORTANT:\n",
    "• The CAPTION was only input context for another model. Do NOT treat it as truth.\n",
    "• Ignore any misleading or incorrect information in the CAPTION.\n",
    "• Use ONLY the IMAGE + the QUESTION + world knowledge to judge correctness.\n",
    "• If the MODEL_ANSWER matches the visible truth in the image, output \"1\".\n",
    "• If the MODEL_ANSWER is wrong, unsupported, or contradicts the image, output \"0\".\n",
    "• Your ENTIRE reply MUST be exactly one character: \"0\" or \"1\".\n",
    "\n",
    "Now evaluate:\n",
    "\n",
    "CAPTION (context only): {caption}\n",
    "CONDITION: {condition}\n",
    "QUESTION: {question}\n",
    "MODEL_ANSWER: {model_answer}\n",
    "\n",
    "Reply with ONLY \"0\" or \"1\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(image_path, caption, condition, question, model_answer):\n",
    "    \"\"\"\n",
    "    Builds the judge prompt, encodes the image,\n",
    "    calls Claude directly, and returns 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build prompt ----\n",
    "    prompt = create_eval_prompt(caption, condition, question, model_answer)\n",
    "\n",
    "    # ---- Encode image ----\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64img = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # ---- Call Claude ----\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": b64img\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---- Parse output ----\n",
    "    output = response.content[0].text.strip()\n",
    "\n",
    "    if output not in (\"0\", \"1\"):\n",
    "        raise ValueError(f\"Unexpected Claude judge output: {output}\")\n",
    "\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to setup eval metric calculation\n",
    "\n",
    "def pair_stats_by_level(jsonl_path):\n",
    "    levels = [\"L0\", \"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "    # Tallies per level\n",
    "    pair_stats = {\n",
    "        lvl: {(1,1):0, (1,0):0, (0,1):0, (0,0):0}\n",
    "        for lvl in levels\n",
    "    }\n",
    "\n",
    "    # ---- Single JSONL pass ----\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            eval_scores = item[\"eval_scores\"]\n",
    "\n",
    "            for lvl in levels:\n",
    "                s_c = eval_scores[lvl][\"correct_caption_score\"]\n",
    "                s_i = eval_scores[lvl][\"incorrect_caption_score\"]\n",
    "                pair_stats[lvl][(s_c, s_i)] += 1\n",
    "\n",
    "    return pair_stats\n",
    "\n",
    "def conf_pairs_by_level(pair_stats):\n",
    "    return pair_stats  # already exactly the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metric - fooling rate by level \n",
    "# When the model answers correctly in the correct-caption condition but answers incorrectly in the incorrect-caption condition.\n",
    "\n",
    "def fooling_rate_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c10 = counts[(1,0)]\n",
    "        total = sum(counts.values())\n",
    "        rate = c10 / total if total > 0 else 0\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"fooled\": c10,\n",
    "            \"total\": total,\n",
    "            \"rate\": rate,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metrics - per-level answer accuracy, computed separately for the correct-caption and incorrect-caption conditions.\n",
    "\n",
    "def acc_by_level(pair_stats):\n",
    "    results = {}\n",
    "\n",
    "    for lvl, counts in pair_stats.items():\n",
    "        c11 = counts[(1,1)]\n",
    "        c10 = counts[(1,0)]\n",
    "        c01 = counts[(0,1)]\n",
    "        c00 = counts[(0,0)]\n",
    "        total = c11 + c10 + c01 + c00\n",
    "\n",
    "        if total == 0:\n",
    "            results[lvl] = None\n",
    "            continue\n",
    "\n",
    "        # accuracy under correct caption = model is correct (regardless of incorrect-caption score)\n",
    "        acc_correct = (c11 + c10) / total\n",
    "        # accuracy under incorrect caption = model is correct under wrong caption\n",
    "        acc_incorrect = (c11 + c01) / total\n",
    "\n",
    "#         mdi = acc_correct - acc_incorrect\n",
    "\n",
    "        results[lvl] = {\n",
    "            \"accuracy_correct_caption\": acc_correct,\n",
    "            \"accuracy_incorrect_caption\": acc_incorrect\n",
    "#             \"MDI\": mdi,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a99bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# V2\n",
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_BLIP_outputs(subset_size=None):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(BLIP_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                \n",
    "                history_correct = [] \n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "                    ans, mdi, attn = ask_blip2(path, correct_caption, q, history=history_correct, return_mdi=True, return_attn=True)\n",
    "                    \n",
    "                    history_correct.append((q, ans))\n",
    "\n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    # entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "                answers_incorrect = {}\n",
    "                mdi_incorrect = {}\n",
    "                entropy_incorrect = {}\n",
    "                shift_incorrect = {}\n",
    "\n",
    "                history_incorrect = []\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "                    ans, mdi, attn = ask_blip2(path, incorrect_caption, q, history=history_incorrect, return_mdi=True, return_attn=True)\n",
    "                    \n",
    "                    history_incorrect.append((q, ans))\n",
    "\n",
    "                    answers_incorrect[lvl] = ans\n",
    "                    mdi_incorrect[lvl] = round(mdi, 3)\n",
    "\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_incorrect[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    if prev_attn is None:\n",
    "                        shift_incorrect[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_incorrect[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    }, \n",
    "                    \n",
    "                    \"mdi_scores\": { #ADDED\n",
    "                        \"correct_caption\": mdi_correct,    #ADDED \n",
    "                        \"incorrect_caption\": mdi_incorrect #ADDED\n",
    "                    },\n",
    "    \n",
    "                    \"entropy_scores\": {\n",
    "                        \"correct_caption\": entropy_correct,\n",
    "                        \"incorrect_caption\": entropy_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"shift_scores\": {\n",
    "                        \"correct_caption\": shift_correct,\n",
    "                        \"incorrect_caption\": shift_incorrect\n",
    "                    },\n",
    "                    \n",
    "                    \"eval_scores\": {}   # will be filled next\n",
    "                }\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {BLIP_OUTPUT_PATH}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2\n",
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_BLIP_outputs(subset_size=None):\n",
    "    all_image_files = [\n",
    "        f for f in os.listdir(IMAGE_FOLDER)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        image_files = random.sample(all_image_files, subset_size)\n",
    "    else:\n",
    "        image_files = all_image_files\n",
    "\n",
    "    print(f\"Found {len(image_files)} images.\\n\")\n",
    "\n",
    "    with open(BLIP_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        for img_file in tqdm(image_files, desc=\"Processing\"):\n",
    "            image_id = os.path.splitext(img_file)[0]\n",
    "            path = os.path.join(IMAGE_FOLDER, img_file)\n",
    "\n",
    "            try:\n",
    "                # ---- 1) GPT captions + questions ----\n",
    "                b64 = encode_image(path)\n",
    "                q = generate_questions(b64)\n",
    "\n",
    "                correct_caption = q[\"correct_caption\"]\n",
    "                incorrect_caption = q[\"incorrect_caption\"]\n",
    "\n",
    "                L0 = q[\"L0\"]\n",
    "                L1 = q[\"L1\"]\n",
    "                L2 = q[\"L2\"]\n",
    "                L3 = q[\"L3\"]\n",
    "\n",
    "                answers_correct = {}\n",
    "                mdi_correct = {}\n",
    "                entropy_correct = {}\n",
    "                shift_correct = {}\n",
    "                \n",
    "                history_correct = [] \n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "                    ans, mdi, attn = ask_blip2(path, correct_caption, q, history=history_correct, return_mdi=True, return_attn=True)\n",
    "                    \n",
    "                    history_correct = [(q, ans)]\n",
    "\n",
    "                    answers_correct[lvl] = ans\n",
    "                    mdi_correct[lvl] = round(mdi, 3)\n",
    "\n",
    "                    # entropy\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_correct[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    # attention shift\n",
    "                    if prev_attn is None:\n",
    "                        shift_correct[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_correct[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "                \n",
    "                answers_incorrect = {}\n",
    "                mdi_incorrect = {}\n",
    "                entropy_incorrect = {}\n",
    "                shift_incorrect = {}\n",
    "\n",
    "                history_incorrect = []\n",
    "                prev_attn = None\n",
    "\n",
    "                for lvl, q in [(\"L0\", L0), (\"L1\", L1), (\"L2\", L2), (\"L3\", L3)]:\n",
    "                    ans, mdi, attn = ask_blip2(path, incorrect_caption, q, history=history_incorrect, return_mdi=True, return_attn=True)\n",
    "                    \n",
    "                    history_incorrect = [(q, ans)]\n",
    "\n",
    "                    answers_incorrect[lvl] = ans\n",
    "                    mdi_incorrect[lvl] = round(mdi, 3)\n",
    "\n",
    "                    ent = compute_attention_entropy(attn)\n",
    "                    entropy_incorrect[lvl] = round(ent, 3) if ent is not None else None\n",
    "\n",
    "                    if prev_attn is None:\n",
    "                        shift_incorrect[lvl] = None\n",
    "                    else:\n",
    "                        shift = compute_attention_shift(prev_attn, attn)\n",
    "                        shift_incorrect[lvl] = round(shift, 3) if shift is not None else None\n",
    "\n",
    "                    prev_attn = attn\n",
    "\n",
    "\n",
    "                # ---- 3) Base JSON structure ----\n",
    "                output = {\n",
    "                    \"image_id\": image_id,\n",
    "\n",
    "                    \"captions\": {\n",
    "                        \"correct\": correct_caption,\n",
    "                        \"incorrect\": incorrect_caption\n",
    "                    },\n",
    "\n",
    "                    \"questions\": {\n",
    "                        \"L0\": L0,\n",
    "                        \"L1\": L1,\n",
    "                        \"L2\": L2,\n",
    "                        \"L3\": L3\n",
    "                    },\n",
    "\n",
    "                    \"answers\": {\n",
    "                        \"correct_caption\": answers_correct,\n",
    "                        \"incorrect_caption\": answers_incorrect\n",
    "                    }, \n",
    "                    \n",
    "                    \"mdi_scores\": { #ADDED\n",
    "                        \"correct_caption\": mdi_correct,    #ADDED \n",
    "                        \"incorrect_caption\": mdi_incorrect #ADDED\n",
    "                    },\n",
    "    \n",
    "                    \"entropy_scores\": {\n",
    "                        \"correct_caption\": entropy_correct,\n",
    "                        \"incorrect_caption\": entropy_incorrect\n",
    "                    },\n",
    "\n",
    "                    \"shift_scores\": {\n",
    "                        \"correct_caption\": shift_correct,\n",
    "                        \"incorrect_caption\": shift_incorrect\n",
    "                    },\n",
    "                    \n",
    "                    \"eval_scores\": {}   # will be filled next\n",
    "                }\n",
    "                \n",
    "\n",
    "                # ---- 4) Parallel Claude evaluation ----\n",
    "                jobs = []\n",
    "                with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                    for level, question in output[\"questions\"].items():\n",
    "\n",
    "                        # correct caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"correct\"],\n",
    "                            \"correct caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"correct_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                        # incorrect caption condition\n",
    "                        jobs.append(ex.submit(\n",
    "                            eval_answer,\n",
    "                            path,\n",
    "                            output[\"captions\"][\"incorrect\"],\n",
    "                            \"incorrect caption condition\",\n",
    "                            question,\n",
    "                            output[\"answers\"][\"incorrect_caption\"][level]\n",
    "                        ))\n",
    "\n",
    "                    # collect results\n",
    "                    ordered_results = [j.result() for j in jobs]\n",
    "\n",
    "                # ---- 5) Attach scores to JSON in correct structure ----\n",
    "                idx = 0\n",
    "                for level in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "                    score_c = ordered_results[idx]; idx += 1\n",
    "                    score_i = ordered_results[idx]; idx += 1\n",
    "\n",
    "                    output[\"eval_scores\"][level] = {\n",
    "                        \"correct_caption_score\": score_c,\n",
    "                        \"incorrect_caption_score\": score_i\n",
    "                    }\n",
    "\n",
    "                # ---- 6) Write one JSON line ----\n",
    "                out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with {image_id}: {e}\")\n",
    "                \n",
    "\n",
    "    print(f\"\\nDone. JSONL saved to: {BLIP_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aade943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-modal comparison (will be implemented in another file)\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def jsonl_to_df(jsonl_path):\n",
    "#     \"\"\"\n",
    "#     Reads your blip_responses.jsonl or llava_responses.jsonl file\n",
    "#     and converts it into a clean pandas DataFrame.\n",
    "    \n",
    "#     Handles:\n",
    "#     - JSONL line-by-line format\n",
    "#     - correct/incorrect caption scores\n",
    "#     - nested mdi/entropy/shift dicts\n",
    "#     - missing/null values safely\n",
    "#     \"\"\"\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     with open(jsonl_path, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "\n",
    "#             # Load current JSON object (one per line)\n",
    "#             entry = json.loads(line)\n",
    "\n",
    "#             image_id = entry[\"image_id\"]\n",
    "\n",
    "#             # Loop over correct_caption / incorrect_caption\n",
    "#             for caption_type in [\"correct_caption\", \"incorrect_caption\"]:\n",
    "\n",
    "#                 # Loop over L0/L1/L2/L3 levels\n",
    "#                 for lvl in [\"L0\", \"L1\", \"L2\", \"L3\"]:\n",
    "\n",
    "#                     # Safely extract nested fields with .get()\n",
    "#                     mdi_val = entry[\"mdi_scores\"][caption_type].get(lvl)\n",
    "#                     entropy_val = entry[\"entropy_scores\"][caption_type].get(lvl)\n",
    "#                     shift_val = entry[\"shift_scores\"][caption_type].get(lvl)\n",
    "\n",
    "#                     # Evaluation: 0 or 1\n",
    "#                     correct_val = entry[\"eval_scores\"][caption_type].get(\n",
    "#                         f\"{caption_type}_score\"\n",
    "#                     )\n",
    "\n",
    "#                     row = {\n",
    "#                         \"image_id\": image_id,\n",
    "#                         \"caption_type\": caption_type,\n",
    "#                         \"level\": lvl,\n",
    "#                         \"mdi\": mdi_val,\n",
    "#                         \"entropy\": entropy_val,\n",
    "#                         \"shift\": shift_val,\n",
    "#                         \"correct\": correct_val,\n",
    "#                     }\n",
    "\n",
    "#                     rows.append(row)\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     # Convert None to NaN (cleaner for plotting)\n",
    "#     df = df.replace({None: np.nan})\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ######### BLIP #########\n",
    "    \n",
    "    #Generates dataset used (correct/incorrect captions, L0-L4 questions)\n",
    "    #Evaluates BLIP-2 responses via Claude Sonnet 4.5 (0 - incorrect; 1 - correct)\n",
    "    generate_BLIP_outputs(subset_size=10) \n",
    "    \n",
    "    # Compute metrics for BLIP responses\n",
    "    BLIP_pair_stats = pair_stats_by_level(BLIP_OUTPUT_PATH)\n",
    "    BLIP_fooling_rate_per_level = fooling_rate_by_level(BLIP_pair_stats)\n",
    "    BLIP_acc_per_level = acc_by_level(BLIP_pair_stats)\n",
    "    \n",
    "    print(\"\\n========================\")\n",
    "    print(\"FOOLING RATE PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in BLIP_fooling_rate_per_level.items():\n",
    "        print(f\"{lvl}: Fooling Rate = {stats['fooled']}/{stats['total']} \"\n",
    "              f\"({stats['rate']:.2f})\")\n",
    "\n",
    "    print(\"\\n========================\")\n",
    "    print(\"ACCURACY PER LEVEL\")\n",
    "    print(\"========================\\n\")\n",
    "    for lvl, stats in BLIP_acc_per_level.items():\n",
    "        print(f\"{lvl}:  \"\n",
    "            f\"Acc(correct caption) = {stats['accuracy_correct_caption']:.2f},  \"\n",
    "            f\"Acc(incorrect caption) = {stats['accuracy_incorrect_caption']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
